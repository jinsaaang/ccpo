{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bf3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from scipy.linalg import svd\n",
    "import torch.optim as optim\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_provider.data_loader_dinn import Dataset_SNP, Dataset_DOW\n",
    "from models.fcn import MLP\n",
    "from models.DLinear import DLinear\n",
    "from utils.bootstrap import make_bootstrap_loader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd3f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snp = pd.read_csv('./datas/snp500.csv')\n",
    "df_dow = pd.read_csv('./datas/dow30.csv')\n",
    "\n",
    "\n",
    "train_end_flag = '2017-12-31'\n",
    "valid_end_flag = '2019-12-31'\n",
    "\n",
    "df_snp_val = df_snp[(df_snp['Date']>train_end_flag) & (df_snp['Date']<=valid_end_flag)][:-4]   \n",
    "df_dow_val = df_dow[(df_dow['Date']>train_end_flag)][:-4]\n",
    "timestamps = df_dow_val['Date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b923d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset_SNP(root_path='./datas/', flag='train', timeenc=1, freq='B', train_end=train_end_flag, val_end=valid_end_flag)\n",
    "valid_set = Dataset_SNP(root_path='./datas/', flag='val', timeenc=1, freq='B', train_end=train_end_flag, val_end=valid_end_flag)\n",
    "test_set = Dataset_SNP(root_path='./datas/', flag='test', timeenc=1, freq='B', train_end=train_end_flag, val_end=train_end_flag)        # valid set에서도 학습할 게 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2588660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = Dataset_DOW(root_path='./datas/', flag='train', timeenc=1, freq='B', train_end=date_flag, val_end=date_flag)\n",
    "# valid_set = Dataset_DOW(root_path='./datas/', flag='val', timeenc=1, freq='B', train_end=date_flag, val_end=date_flag)\n",
    "# test_set = Dataset_DOW(root_path='./datas/', flag='test', timeenc=1, freq='B', train_end=date_flag, val_end=date_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1f0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "bootstrap_loaders = make_bootstrap_loader(train_set, B=30, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fa61e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLinearConfig:\n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual=False):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        self.individual = individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(model_cls, data_loader, valid_data_loader, EPOCHS=100, lr=1e-3, path='./weights/', patience=10, valid_mode = False): \n",
    "    \n",
    "    models = []\n",
    "    indices_ls = []\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for i, (loader_b, indices_b) in enumerate(data_loader):\n",
    "        \n",
    "        if model_cls == MLP:\n",
    "            sample_x, sample_y, _, _ = next(iter(loader_b))\n",
    "            input_dim = sample_x.shape[1] * sample_x.shape[2]\n",
    "            hidden_dim = 2000\n",
    "            output_dim = sample_y.shape[1] * sample_y.shape[2]\n",
    "            model_b = model_cls(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        \n",
    "        elif model_cls == DLinear:\n",
    "            sample_x, sample_y, _, _ = next(iter(loader_b))\n",
    "            seq_len = sample_x.shape[1]\n",
    "            pred_len = sample_y.shape[1]\n",
    "            enc_in = sample_x.shape[2]\n",
    "            dlinear_configs = DLinearConfig(seq_len=seq_len, pred_len=pred_len, enc_in=enc_in)\n",
    "            model_b = model_cls(dlinear_configs).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model_b.parameters(), lr=lr)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            model_b.train()\n",
    "            total_train_loss = 0.0 \n",
    "            for X_batch, y_batch, _, _ in loader_b:\n",
    "                \n",
    "                if model_cls == MLP:\n",
    "                    X_batch = X_batch.float().to(device).view(X_batch.size(0), -1)\n",
    "                    y_batch = y_batch.float().to(device).view(y_batch.size(0), -1)\n",
    "                elif model_cls == DLinear:\n",
    "                    X_batch = X_batch.float().to(device)\n",
    "                    y_batch = y_batch.float().to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                preds = model_b(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(loader_b)\n",
    "            print(f\"Model Num {i}, Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            \n",
    "            if valid_mode:\n",
    "                model_b.eval()\n",
    "                total_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_val, y_val, _, _ in valid_data_loader:\n",
    "                        if model_cls == MLP:\n",
    "                            X_val = X_val.float().to(device).view(X_val.size(0), -1)\n",
    "                            y_val = y_val.float().to(device).view(y_val.size(0), -1)\n",
    "                        elif model_cls == DLinear:\n",
    "                            X_val = X_val.float().to(device)\n",
    "                            y_val = y_val.float().to(device)\n",
    "                        \n",
    "                        preds_val = model_b(X_val)\n",
    "                        val_loss = criterion(preds_val, y_val)\n",
    "                        total_val_loss += val_loss.item()\n",
    "                \n",
    "                avg_train_loss = total_train_loss / len(loader_b)\n",
    "                avg_val_loss = total_val_loss / len(valid_data_loader)\n",
    "                \n",
    "                print(f\"Model Num {i}, Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_val_loss:.4f}\")\n",
    "            # if (epoch+1) % 10 == 0:\n",
    "            #     print(f\"Model Num {i}, Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    torch.save(model_b.state_dict(), f\"{path}/model_b{i}.pt\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping for model {i} at epoch {epoch+1}!\")\n",
    "                    print()\n",
    "                    break\n",
    "        \n",
    "        model_b.load_state_dict(torch.load(f\"{path}/model_b{i}.pt\"))\n",
    "        models.append(model_b)\n",
    "        indices_ls.append(indices_b)\n",
    "        \n",
    "    return models, indices_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a460a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_cls, data_loader, valid_data_loader, EPOCHS=100, lr=1e-3, path='./weights/', patience=10, valid_mode=False):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    models = []\n",
    "    indices_ls = []\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for i, (loader_b, indices_b) in enumerate(data_loader):\n",
    "        \n",
    "        sample_x, sample_y, _, _ = next(iter(loader_b))\n",
    "\n",
    "        if model_cls == MLP:\n",
    "            input_dim = sample_x.shape[1] * sample_x.shape[2]\n",
    "            hidden_dim = 2000\n",
    "            output_dim = sample_y.shape[1] * sample_y.shape[2]\n",
    "            model_b = model_cls(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        \n",
    "        elif model_cls == DLinear:\n",
    "            seq_len = sample_x.shape[1]\n",
    "            pred_len = sample_y.shape[1]\n",
    "            enc_in = sample_x.shape[2]\n",
    "            dlinear_configs = DLinearConfig(seq_len=seq_len, pred_len=pred_len, enc_in=enc_in)\n",
    "            model_b = model_cls(dlinear_configs).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model class: {model_cls}\")\n",
    "            \n",
    "        optimizer = optim.Adam(model_b.parameters(), lr=lr)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # ================================================================== #\n",
    "        # 아래 부분을 수정하여 파일 이름에 모델 클래스 이름을 포함시킵니다.\n",
    "        # 예: ./weights/MLP_model_b0.pt 또는 ./weights/DLinear_model_b0.pt\n",
    "        model_name = model_cls.__name__ \n",
    "        model_save_path = f\"{path}/{model_name}_model_b{i}.pt\"\n",
    "        # ================================================================== #\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            model_b.train()\n",
    "            total_train_loss = 0.0 \n",
    "            for X_batch, y_batch, _, _ in loader_b:\n",
    "                \n",
    "                if model_cls == MLP:\n",
    "                    X_batch = X_batch.float().to(device).view(X_batch.size(0), -1)\n",
    "                    y_batch = y_batch.float().to(device).view(y_batch.size(0), -1)\n",
    "                elif model_cls == DLinear:\n",
    "                    X_batch = X_batch.float().to(device)\n",
    "                    y_batch = y_batch.float().to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                preds = model_b(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(loader_b)\n",
    "\n",
    "            if valid_mode:\n",
    "                model_b.eval()\n",
    "                total_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_val, y_val, _, _ in valid_data_loader:\n",
    "                        if model_cls == MLP:\n",
    "                            X_val = X_val.float().to(device).view(X_val.size(0), -1)\n",
    "                            y_val = y_val.float().to(device).view(y_val.size(0), -1)\n",
    "                        elif model_cls == DLinear:\n",
    "                            X_val = X_val.float().to(device)\n",
    "                            y_val = y_val.float().to(device)\n",
    "                        \n",
    "                        preds_val = model_b(X_val)\n",
    "                        val_loss = criterion(preds_val, y_val)\n",
    "                        total_val_loss += val_loss.item()\n",
    "                \n",
    "                avg_val_loss = total_val_loss / len(valid_data_loader)\n",
    "                \n",
    "                print(f\"Model Num {i} ({model_name}), Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    torch.save(model_b.state_dict(), model_save_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping for model {i} ({model_name}) at epoch {epoch+1}!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Model Num {i} ({model_name}), Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # 학습이 끝난 후 모델을 불러오거나 저장\n",
    "        if valid_mode:\n",
    "            # 조기 종료된 경우, 파일이 존재하지 않을 수 있으므로 확인 후 로드\n",
    "            if os.path.exists(model_save_path):\n",
    "                print(f\"Loading best model for {model_name}_{i} with validation loss: {best_val_loss:.4f}\")\n",
    "                model_b.load_state_dict(torch.load(model_save_path))\n",
    "            else:\n",
    "                print(f\"Warning: No model was saved for {model_name}_{i} as validation loss never improved.\")\n",
    "        else:\n",
    "            print(f\"Saving final model for {model_name}_{i} after {EPOCHS} epochs.\")\n",
    "            torch.save(model_b.state_dict(), model_save_path)\n",
    "        \n",
    "        models.append(model_b)\n",
    "        indices_ls.append(indices_b)\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    return models, indices_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7979ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Num 0 (MLP), Epoch 1, Train Loss: 0.9673, Valid Loss: 0.8124\n",
      "Model Num 0 (MLP), Epoch 2, Train Loss: 1.0330, Valid Loss: 0.8149\n",
      "Model Num 0 (MLP), Epoch 3, Train Loss: 0.9334, Valid Loss: 0.8272\n",
      "Model Num 0 (MLP), Epoch 4, Train Loss: 1.1150, Valid Loss: 0.8147\n",
      "Model Num 0 (MLP), Epoch 5, Train Loss: 1.5330, Valid Loss: 0.8113\n",
      "Model Num 0 (MLP), Epoch 6, Train Loss: 0.9653, Valid Loss: 0.8127\n",
      "Model Num 0 (MLP), Epoch 7, Train Loss: 0.8792, Valid Loss: 0.8135\n",
      "Model Num 0 (MLP), Epoch 8, Train Loss: 0.8545, Valid Loss: 0.8114\n",
      "Model Num 0 (MLP), Epoch 9, Train Loss: 0.8520, Valid Loss: 0.8099\n",
      "Model Num 0 (MLP), Epoch 10, Train Loss: 0.8589, Valid Loss: 0.8127\n",
      "Model Num 0 (MLP), Epoch 11, Train Loss: 0.8355, Valid Loss: 0.8126\n",
      "Model Num 0 (MLP), Epoch 12, Train Loss: 0.8298, Valid Loss: 0.8122\n",
      "Model Num 0 (MLP), Epoch 13, Train Loss: 0.8197, Valid Loss: 0.8126\n",
      "Model Num 0 (MLP), Epoch 14, Train Loss: 0.8433, Valid Loss: 0.8148\n",
      "Model Num 0 (MLP), Epoch 15, Train Loss: 0.8573, Valid Loss: 0.8164\n",
      "Model Num 0 (MLP), Epoch 16, Train Loss: 1.1273, Valid Loss: 0.8159\n",
      "Model Num 0 (MLP), Epoch 17, Train Loss: 1.2411, Valid Loss: 0.8126\n",
      "Model Num 0 (MLP), Epoch 18, Train Loss: 1.3263, Valid Loss: 0.8199\n",
      "Model Num 0 (MLP), Epoch 19, Train Loss: 1.1673, Valid Loss: 0.8100\n",
      "Early stopping for model 0 (MLP) at epoch 19!\n",
      "Loading best model for MLP_0 with validation loss: 0.8099\n",
      "------------------------------------------------------------\n",
      "Model Num 1 (MLP), Epoch 1, Train Loss: 0.9771, Valid Loss: 0.8173\n",
      "Model Num 1 (MLP), Epoch 2, Train Loss: 0.9614, Valid Loss: 0.8548\n",
      "Model Num 1 (MLP), Epoch 3, Train Loss: 1.0097, Valid Loss: 0.8099\n",
      "Model Num 1 (MLP), Epoch 4, Train Loss: 0.9521, Valid Loss: 0.8140\n",
      "Model Num 1 (MLP), Epoch 5, Train Loss: 0.9047, Valid Loss: 0.8125\n",
      "Model Num 1 (MLP), Epoch 6, Train Loss: 0.9537, Valid Loss: 0.8093\n",
      "Model Num 1 (MLP), Epoch 7, Train Loss: 0.8989, Valid Loss: 0.8119\n",
      "Model Num 1 (MLP), Epoch 8, Train Loss: 0.9565, Valid Loss: 0.8150\n",
      "Model Num 1 (MLP), Epoch 9, Train Loss: 3.5484, Valid Loss: 0.8133\n",
      "Model Num 1 (MLP), Epoch 10, Train Loss: 2.9917, Valid Loss: 0.8152\n",
      "Model Num 1 (MLP), Epoch 11, Train Loss: 1.2279, Valid Loss: 0.8134\n",
      "Model Num 1 (MLP), Epoch 12, Train Loss: 0.8964, Valid Loss: 0.8132\n",
      "Model Num 1 (MLP), Epoch 13, Train Loss: 0.8515, Valid Loss: 0.8124\n",
      "Model Num 1 (MLP), Epoch 14, Train Loss: 0.8615, Valid Loss: 0.8137\n",
      "Model Num 1 (MLP), Epoch 15, Train Loss: 0.8724, Valid Loss: 0.8132\n",
      "Model Num 1 (MLP), Epoch 16, Train Loss: 0.8542, Valid Loss: 0.8192\n",
      "Early stopping for model 1 (MLP) at epoch 16!\n",
      "Loading best model for MLP_1 with validation loss: 0.8093\n",
      "------------------------------------------------------------\n",
      "Model Num 2 (MLP), Epoch 1, Train Loss: 1.0264, Valid Loss: 0.8164\n",
      "Model Num 2 (MLP), Epoch 2, Train Loss: 0.9804, Valid Loss: 0.8172\n",
      "Model Num 2 (MLP), Epoch 3, Train Loss: 0.9831, Valid Loss: 0.8149\n",
      "Model Num 2 (MLP), Epoch 4, Train Loss: 0.9475, Valid Loss: 0.8136\n",
      "Model Num 2 (MLP), Epoch 5, Train Loss: 0.9188, Valid Loss: 0.8142\n",
      "Model Num 2 (MLP), Epoch 6, Train Loss: 0.9194, Valid Loss: 0.8219\n",
      "Model Num 2 (MLP), Epoch 7, Train Loss: 0.9309, Valid Loss: 0.8190\n",
      "Model Num 2 (MLP), Epoch 8, Train Loss: 0.9861, Valid Loss: 0.8170\n",
      "Model Num 2 (MLP), Epoch 9, Train Loss: 0.9395, Valid Loss: 0.8174\n",
      "Model Num 2 (MLP), Epoch 10, Train Loss: 1.3643, Valid Loss: 0.8152\n",
      "Model Num 2 (MLP), Epoch 11, Train Loss: 1.1213, Valid Loss: 0.8156\n",
      "Model Num 2 (MLP), Epoch 12, Train Loss: 1.0065, Valid Loss: 0.8165\n",
      "Model Num 2 (MLP), Epoch 13, Train Loss: 1.5191, Valid Loss: 0.8193\n",
      "Model Num 2 (MLP), Epoch 14, Train Loss: 0.9606, Valid Loss: 0.8185\n",
      "Early stopping for model 2 (MLP) at epoch 14!\n",
      "Loading best model for MLP_2 with validation loss: 0.8136\n",
      "------------------------------------------------------------\n",
      "Model Num 3 (MLP), Epoch 1, Train Loss: 1.0380, Valid Loss: 0.8157\n",
      "Model Num 3 (MLP), Epoch 2, Train Loss: 0.9816, Valid Loss: 0.8213\n",
      "Model Num 3 (MLP), Epoch 3, Train Loss: 0.9928, Valid Loss: 0.8099\n",
      "Model Num 3 (MLP), Epoch 4, Train Loss: 0.9581, Valid Loss: 0.8111\n",
      "Model Num 3 (MLP), Epoch 5, Train Loss: 0.9370, Valid Loss: 0.8167\n",
      "Model Num 3 (MLP), Epoch 6, Train Loss: 0.9458, Valid Loss: 0.8107\n",
      "Model Num 3 (MLP), Epoch 7, Train Loss: 0.9517, Valid Loss: 0.8134\n",
      "Model Num 3 (MLP), Epoch 8, Train Loss: 1.0283, Valid Loss: 0.8219\n",
      "Model Num 3 (MLP), Epoch 9, Train Loss: 0.9727, Valid Loss: 0.8159\n",
      "Model Num 3 (MLP), Epoch 10, Train Loss: 0.9721, Valid Loss: 0.8107\n",
      "Model Num 3 (MLP), Epoch 11, Train Loss: 1.0502, Valid Loss: 0.8107\n",
      "Model Num 3 (MLP), Epoch 12, Train Loss: 0.9051, Valid Loss: 0.8116\n",
      "Model Num 3 (MLP), Epoch 13, Train Loss: 0.9161, Valid Loss: 0.8126\n",
      "Early stopping for model 3 (MLP) at epoch 13!\n",
      "Loading best model for MLP_3 with validation loss: 0.8099\n",
      "------------------------------------------------------------\n",
      "Model Num 4 (MLP), Epoch 1, Train Loss: 1.0800, Valid Loss: 0.8285\n",
      "Model Num 4 (MLP), Epoch 2, Train Loss: 1.0745, Valid Loss: 0.8320\n",
      "Model Num 4 (MLP), Epoch 3, Train Loss: 0.9644, Valid Loss: 0.8162\n",
      "Model Num 4 (MLP), Epoch 4, Train Loss: 0.9393, Valid Loss: 0.8148\n",
      "Model Num 4 (MLP), Epoch 5, Train Loss: 0.9756, Valid Loss: 0.8098\n",
      "Model Num 4 (MLP), Epoch 6, Train Loss: 0.9250, Valid Loss: 0.8092\n",
      "Model Num 4 (MLP), Epoch 7, Train Loss: 0.9509, Valid Loss: 0.8116\n",
      "Model Num 4 (MLP), Epoch 8, Train Loss: 0.9727, Valid Loss: 0.8124\n",
      "Model Num 4 (MLP), Epoch 9, Train Loss: 1.2292, Valid Loss: 0.8108\n",
      "Model Num 4 (MLP), Epoch 10, Train Loss: 1.1843, Valid Loss: 0.8153\n",
      "Model Num 4 (MLP), Epoch 11, Train Loss: 2.0190, Valid Loss: 0.8138\n",
      "Model Num 4 (MLP), Epoch 12, Train Loss: 1.1086, Valid Loss: 0.8130\n",
      "Model Num 4 (MLP), Epoch 13, Train Loss: 1.3348, Valid Loss: 0.8125\n",
      "Model Num 4 (MLP), Epoch 14, Train Loss: 8.4923, Valid Loss: 0.8145\n",
      "Model Num 4 (MLP), Epoch 15, Train Loss: 1.9279, Valid Loss: 0.8133\n",
      "Model Num 4 (MLP), Epoch 16, Train Loss: 1.5508, Valid Loss: 0.8194\n",
      "Early stopping for model 4 (MLP) at epoch 16!\n",
      "Loading best model for MLP_4 with validation loss: 0.8092\n",
      "------------------------------------------------------------\n",
      "Model Num 5 (MLP), Epoch 1, Train Loss: 0.9973, Valid Loss: 0.8146\n",
      "Model Num 5 (MLP), Epoch 2, Train Loss: 0.9531, Valid Loss: 0.8350\n",
      "Model Num 5 (MLP), Epoch 3, Train Loss: 0.9827, Valid Loss: 0.8117\n",
      "Model Num 5 (MLP), Epoch 4, Train Loss: 0.9603, Valid Loss: 0.8131\n",
      "Model Num 5 (MLP), Epoch 5, Train Loss: 0.9512, Valid Loss: 0.8117\n",
      "Model Num 5 (MLP), Epoch 6, Train Loss: 0.9221, Valid Loss: 0.8096\n",
      "Model Num 5 (MLP), Epoch 7, Train Loss: 0.9243, Valid Loss: 0.8094\n",
      "Model Num 5 (MLP), Epoch 8, Train Loss: 0.9555, Valid Loss: 0.8126\n",
      "Model Num 5 (MLP), Epoch 9, Train Loss: 4.0894, Valid Loss: 0.8093\n",
      "Model Num 5 (MLP), Epoch 10, Train Loss: 1.7074, Valid Loss: 0.8114\n",
      "Model Num 5 (MLP), Epoch 11, Train Loss: 3.3202, Valid Loss: 0.8085\n",
      "Model Num 5 (MLP), Epoch 12, Train Loss: 3.4525, Valid Loss: 0.8110\n",
      "Model Num 5 (MLP), Epoch 13, Train Loss: 2.4161, Valid Loss: 0.8089\n",
      "Model Num 5 (MLP), Epoch 14, Train Loss: 1.4653, Valid Loss: 0.8104\n",
      "Model Num 5 (MLP), Epoch 15, Train Loss: 1.0762, Valid Loss: 0.8100\n",
      "Model Num 5 (MLP), Epoch 16, Train Loss: 1.2304, Valid Loss: 0.8108\n",
      "Model Num 5 (MLP), Epoch 17, Train Loss: 2.4369, Valid Loss: 0.8112\n",
      "Model Num 5 (MLP), Epoch 18, Train Loss: 2.2333, Valid Loss: 0.8109\n",
      "Model Num 5 (MLP), Epoch 19, Train Loss: 1.1594, Valid Loss: 0.8090\n",
      "Model Num 5 (MLP), Epoch 20, Train Loss: 0.9157, Valid Loss: 0.8143\n",
      "Loading best model for MLP_5 with validation loss: 0.8085\n",
      "------------------------------------------------------------\n",
      "Model Num 6 (MLP), Epoch 1, Train Loss: 1.0207, Valid Loss: 0.8282\n",
      "Model Num 6 (MLP), Epoch 2, Train Loss: 0.9884, Valid Loss: 0.8143\n",
      "Model Num 6 (MLP), Epoch 3, Train Loss: 0.9774, Valid Loss: 0.8148\n",
      "Model Num 6 (MLP), Epoch 4, Train Loss: 0.9571, Valid Loss: 0.8133\n",
      "Model Num 6 (MLP), Epoch 5, Train Loss: 0.9954, Valid Loss: 0.8166\n",
      "Model Num 6 (MLP), Epoch 6, Train Loss: 0.9847, Valid Loss: 0.8212\n",
      "Model Num 6 (MLP), Epoch 7, Train Loss: 1.0541, Valid Loss: 0.8123\n",
      "Model Num 6 (MLP), Epoch 8, Train Loss: 1.7441, Valid Loss: 0.8115\n",
      "Model Num 6 (MLP), Epoch 9, Train Loss: 3.5420, Valid Loss: 0.8107\n",
      "Model Num 6 (MLP), Epoch 10, Train Loss: 1.3251, Valid Loss: 0.8118\n",
      "Model Num 6 (MLP), Epoch 11, Train Loss: 1.0007, Valid Loss: 0.8109\n",
      "Model Num 6 (MLP), Epoch 12, Train Loss: 0.9730, Valid Loss: 0.8123\n",
      "Model Num 6 (MLP), Epoch 13, Train Loss: 0.9605, Valid Loss: 0.8107\n",
      "Model Num 6 (MLP), Epoch 14, Train Loss: 5.8910, Valid Loss: 0.8179\n",
      "Model Num 6 (MLP), Epoch 15, Train Loss: 3.5421, Valid Loss: 0.8131\n",
      "Model Num 6 (MLP), Epoch 16, Train Loss: 2.0983, Valid Loss: 0.8097\n",
      "Model Num 6 (MLP), Epoch 17, Train Loss: 0.9902, Valid Loss: 0.8137\n",
      "Model Num 6 (MLP), Epoch 18, Train Loss: 0.9592, Valid Loss: 0.8119\n",
      "Model Num 6 (MLP), Epoch 19, Train Loss: 0.9547, Valid Loss: 0.8134\n",
      "Model Num 6 (MLP), Epoch 20, Train Loss: 0.9398, Valid Loss: 0.8276\n",
      "Loading best model for MLP_6 with validation loss: 0.8097\n",
      "------------------------------------------------------------\n",
      "Model Num 7 (MLP), Epoch 1, Train Loss: 1.0037, Valid Loss: 0.8225\n",
      "Model Num 7 (MLP), Epoch 2, Train Loss: 0.9927, Valid Loss: 0.8122\n",
      "Model Num 7 (MLP), Epoch 3, Train Loss: 0.9447, Valid Loss: 0.8161\n",
      "Model Num 7 (MLP), Epoch 4, Train Loss: 1.1580, Valid Loss: 0.8133\n",
      "Model Num 7 (MLP), Epoch 5, Train Loss: 1.0139, Valid Loss: 0.8253\n",
      "Model Num 7 (MLP), Epoch 6, Train Loss: 0.9458, Valid Loss: 0.8142\n",
      "Model Num 7 (MLP), Epoch 7, Train Loss: 0.9600, Valid Loss: 0.8115\n",
      "Model Num 7 (MLP), Epoch 8, Train Loss: 1.2851, Valid Loss: 0.8113\n",
      "Model Num 7 (MLP), Epoch 9, Train Loss: 2.6165, Valid Loss: 0.8119\n",
      "Model Num 7 (MLP), Epoch 10, Train Loss: 1.3622, Valid Loss: 0.8128\n",
      "Model Num 7 (MLP), Epoch 11, Train Loss: 1.1578, Valid Loss: 0.8140\n",
      "Model Num 7 (MLP), Epoch 12, Train Loss: 1.4028, Valid Loss: 0.8142\n",
      "Model Num 7 (MLP), Epoch 13, Train Loss: 2.5392, Valid Loss: 0.8170\n",
      "Model Num 7 (MLP), Epoch 14, Train Loss: 4.4899, Valid Loss: 0.8096\n",
      "Model Num 7 (MLP), Epoch 15, Train Loss: 1.2929, Valid Loss: 0.8114\n",
      "Model Num 7 (MLP), Epoch 16, Train Loss: 0.9100, Valid Loss: 0.8106\n",
      "Model Num 7 (MLP), Epoch 17, Train Loss: 0.9093, Valid Loss: 0.8102\n",
      "Model Num 7 (MLP), Epoch 18, Train Loss: 0.8740, Valid Loss: 0.8105\n",
      "Model Num 7 (MLP), Epoch 19, Train Loss: 0.8800, Valid Loss: 0.8116\n",
      "Model Num 7 (MLP), Epoch 20, Train Loss: 0.8770, Valid Loss: 0.8136\n",
      "Loading best model for MLP_7 with validation loss: 0.8096\n",
      "------------------------------------------------------------\n",
      "Model Num 8 (MLP), Epoch 1, Train Loss: 1.0174, Valid Loss: 0.8128\n",
      "Model Num 8 (MLP), Epoch 2, Train Loss: 0.9824, Valid Loss: 0.8168\n",
      "Model Num 8 (MLP), Epoch 3, Train Loss: 0.9540, Valid Loss: 0.8168\n",
      "Model Num 8 (MLP), Epoch 4, Train Loss: 0.9445, Valid Loss: 0.8094\n",
      "Model Num 8 (MLP), Epoch 5, Train Loss: 0.9278, Valid Loss: 0.8135\n",
      "Model Num 8 (MLP), Epoch 6, Train Loss: 0.9259, Valid Loss: 0.8084\n",
      "Model Num 8 (MLP), Epoch 7, Train Loss: 1.8712, Valid Loss: 0.8118\n",
      "Model Num 8 (MLP), Epoch 8, Train Loss: 4.6812, Valid Loss: 0.8162\n",
      "Model Num 8 (MLP), Epoch 9, Train Loss: 1.0209, Valid Loss: 0.8183\n",
      "Model Num 8 (MLP), Epoch 10, Train Loss: 0.9647, Valid Loss: 0.8121\n",
      "Model Num 8 (MLP), Epoch 11, Train Loss: 0.9501, Valid Loss: 0.8099\n",
      "Model Num 8 (MLP), Epoch 12, Train Loss: 0.9690, Valid Loss: 0.8151\n",
      "Model Num 8 (MLP), Epoch 13, Train Loss: 0.9582, Valid Loss: 0.8091\n",
      "Model Num 8 (MLP), Epoch 14, Train Loss: 0.9006, Valid Loss: 0.8122\n",
      "Model Num 8 (MLP), Epoch 15, Train Loss: 2.3773, Valid Loss: 0.8126\n",
      "Model Num 8 (MLP), Epoch 16, Train Loss: 1.9561, Valid Loss: 0.8110\n",
      "Early stopping for model 8 (MLP) at epoch 16!\n",
      "Loading best model for MLP_8 with validation loss: 0.8084\n",
      "------------------------------------------------------------\n",
      "Model Num 9 (MLP), Epoch 1, Train Loss: 1.0113, Valid Loss: 0.8241\n",
      "Model Num 9 (MLP), Epoch 2, Train Loss: 0.9946, Valid Loss: 0.8161\n",
      "Model Num 9 (MLP), Epoch 3, Train Loss: 0.9566, Valid Loss: 0.8127\n",
      "Model Num 9 (MLP), Epoch 4, Train Loss: 0.9462, Valid Loss: 0.8163\n",
      "Model Num 9 (MLP), Epoch 5, Train Loss: 0.9453, Valid Loss: 0.8137\n",
      "Model Num 9 (MLP), Epoch 6, Train Loss: 0.9762, Valid Loss: 0.8113\n",
      "Model Num 9 (MLP), Epoch 7, Train Loss: 0.9229, Valid Loss: 0.8190\n",
      "Model Num 9 (MLP), Epoch 8, Train Loss: 0.9477, Valid Loss: 0.8158\n",
      "Model Num 9 (MLP), Epoch 9, Train Loss: 0.9496, Valid Loss: 0.8125\n",
      "Model Num 9 (MLP), Epoch 10, Train Loss: 1.1409, Valid Loss: 0.8144\n",
      "Model Num 9 (MLP), Epoch 11, Train Loss: 2.2388, Valid Loss: 0.8143\n",
      "Model Num 9 (MLP), Epoch 12, Train Loss: 1.0737, Valid Loss: 0.8169\n",
      "Model Num 9 (MLP), Epoch 13, Train Loss: 1.3487, Valid Loss: 0.8113\n",
      "Model Num 9 (MLP), Epoch 14, Train Loss: 1.5515, Valid Loss: 0.8119\n",
      "Model Num 9 (MLP), Epoch 15, Train Loss: 2.4894, Valid Loss: 0.8228\n",
      "Model Num 9 (MLP), Epoch 16, Train Loss: 2.9955, Valid Loss: 0.8105\n",
      "Model Num 9 (MLP), Epoch 17, Train Loss: 1.9814, Valid Loss: 0.8122\n",
      "Model Num 9 (MLP), Epoch 18, Train Loss: 0.9807, Valid Loss: 0.8136\n",
      "Model Num 9 (MLP), Epoch 19, Train Loss: 1.0828, Valid Loss: 0.8159\n",
      "Model Num 9 (MLP), Epoch 20, Train Loss: 1.7032, Valid Loss: 0.8124\n",
      "Loading best model for MLP_9 with validation loss: 0.8105\n",
      "------------------------------------------------------------\n",
      "Model Num 10 (MLP), Epoch 1, Train Loss: 0.9991, Valid Loss: 0.8156\n",
      "Model Num 10 (MLP), Epoch 2, Train Loss: 0.9450, Valid Loss: 0.8309\n",
      "Model Num 10 (MLP), Epoch 3, Train Loss: 0.9866, Valid Loss: 0.8116\n",
      "Model Num 10 (MLP), Epoch 4, Train Loss: 0.9050, Valid Loss: 0.8129\n",
      "Model Num 10 (MLP), Epoch 5, Train Loss: 0.8906, Valid Loss: 0.8147\n",
      "Model Num 10 (MLP), Epoch 6, Train Loss: 0.9031, Valid Loss: 0.8103\n",
      "Model Num 10 (MLP), Epoch 7, Train Loss: 0.8700, Valid Loss: 0.8189\n",
      "Model Num 10 (MLP), Epoch 8, Train Loss: 1.0264, Valid Loss: 0.8339\n",
      "Model Num 10 (MLP), Epoch 9, Train Loss: 2.7141, Valid Loss: 0.8223\n",
      "Model Num 10 (MLP), Epoch 10, Train Loss: 2.0957, Valid Loss: 0.8118\n",
      "Model Num 10 (MLP), Epoch 11, Train Loss: 0.9409, Valid Loss: 0.8147\n",
      "Model Num 10 (MLP), Epoch 12, Train Loss: 3.4623, Valid Loss: 0.8108\n",
      "Model Num 10 (MLP), Epoch 13, Train Loss: 3.8188, Valid Loss: 0.8098\n",
      "Model Num 10 (MLP), Epoch 14, Train Loss: 0.8800, Valid Loss: 0.8130\n",
      "Model Num 10 (MLP), Epoch 15, Train Loss: 0.8411, Valid Loss: 0.8134\n",
      "Model Num 10 (MLP), Epoch 16, Train Loss: 0.8904, Valid Loss: 0.8104\n",
      "Model Num 10 (MLP), Epoch 17, Train Loss: 0.8442, Valid Loss: 0.8132\n",
      "Model Num 10 (MLP), Epoch 18, Train Loss: 0.8688, Valid Loss: 0.8118\n",
      "Model Num 10 (MLP), Epoch 19, Train Loss: 0.8525, Valid Loss: 0.8116\n",
      "Model Num 10 (MLP), Epoch 20, Train Loss: 0.8161, Valid Loss: 0.8140\n",
      "Loading best model for MLP_10 with validation loss: 0.8098\n",
      "------------------------------------------------------------\n",
      "Model Num 11 (MLP), Epoch 1, Train Loss: 0.9878, Valid Loss: 0.8165\n",
      "Model Num 11 (MLP), Epoch 2, Train Loss: 0.9423, Valid Loss: 0.8113\n",
      "Model Num 11 (MLP), Epoch 3, Train Loss: 0.9403, Valid Loss: 0.8283\n",
      "Model Num 11 (MLP), Epoch 4, Train Loss: 0.8968, Valid Loss: 0.8221\n",
      "Model Num 11 (MLP), Epoch 5, Train Loss: 0.9116, Valid Loss: 0.8183\n",
      "Model Num 11 (MLP), Epoch 6, Train Loss: 0.8971, Valid Loss: 0.8179\n",
      "Model Num 11 (MLP), Epoch 7, Train Loss: 0.9044, Valid Loss: 0.8207\n",
      "Model Num 11 (MLP), Epoch 8, Train Loss: 0.9434, Valid Loss: 0.8310\n",
      "Model Num 11 (MLP), Epoch 9, Train Loss: 0.9937, Valid Loss: 0.8137\n",
      "Model Num 11 (MLP), Epoch 10, Train Loss: 0.9780, Valid Loss: 0.8187\n",
      "Model Num 11 (MLP), Epoch 11, Train Loss: 1.9962, Valid Loss: 0.8269\n",
      "Model Num 11 (MLP), Epoch 12, Train Loss: 2.3869, Valid Loss: 0.8118\n",
      "Early stopping for model 11 (MLP) at epoch 12!\n",
      "Loading best model for MLP_11 with validation loss: 0.8113\n",
      "------------------------------------------------------------\n",
      "Model Num 12 (MLP), Epoch 1, Train Loss: 0.9881, Valid Loss: 0.8268\n",
      "Model Num 12 (MLP), Epoch 2, Train Loss: 0.9813, Valid Loss: 0.8148\n",
      "Model Num 12 (MLP), Epoch 3, Train Loss: 0.9402, Valid Loss: 0.8173\n",
      "Model Num 12 (MLP), Epoch 4, Train Loss: 0.9103, Valid Loss: 0.8175\n",
      "Model Num 12 (MLP), Epoch 5, Train Loss: 3.6186, Valid Loss: 0.8172\n",
      "Model Num 12 (MLP), Epoch 6, Train Loss: 2.0537, Valid Loss: 0.8215\n",
      "Model Num 12 (MLP), Epoch 7, Train Loss: 0.8975, Valid Loss: 0.8133\n",
      "Model Num 12 (MLP), Epoch 8, Train Loss: 0.8799, Valid Loss: 0.8142\n",
      "Model Num 12 (MLP), Epoch 9, Train Loss: 0.8801, Valid Loss: 0.8152\n",
      "Model Num 12 (MLP), Epoch 10, Train Loss: 0.9163, Valid Loss: 0.8120\n",
      "Model Num 12 (MLP), Epoch 11, Train Loss: 1.9398, Valid Loss: 0.8203\n",
      "Model Num 12 (MLP), Epoch 12, Train Loss: 3.8920, Valid Loss: 0.8142\n",
      "Model Num 12 (MLP), Epoch 13, Train Loss: 1.1229, Valid Loss: 0.8104\n",
      "Model Num 12 (MLP), Epoch 14, Train Loss: 0.9137, Valid Loss: 0.8107\n",
      "Model Num 12 (MLP), Epoch 15, Train Loss: 0.9045, Valid Loss: 0.8221\n",
      "Model Num 12 (MLP), Epoch 16, Train Loss: 0.9629, Valid Loss: 0.8107\n",
      "Model Num 12 (MLP), Epoch 17, Train Loss: 0.9012, Valid Loss: 0.8141\n",
      "Model Num 12 (MLP), Epoch 18, Train Loss: 0.8765, Valid Loss: 0.8131\n",
      "Model Num 12 (MLP), Epoch 19, Train Loss: 0.8547, Valid Loss: 0.8168\n",
      "Model Num 12 (MLP), Epoch 20, Train Loss: 0.9110, Valid Loss: 0.8152\n",
      "Loading best model for MLP_12 with validation loss: 0.8104\n",
      "------------------------------------------------------------\n",
      "Model Num 13 (MLP), Epoch 1, Train Loss: 1.0438, Valid Loss: 0.8149\n",
      "Model Num 13 (MLP), Epoch 2, Train Loss: 1.0176, Valid Loss: 0.8183\n",
      "Model Num 13 (MLP), Epoch 3, Train Loss: 0.9633, Valid Loss: 0.8255\n",
      "Model Num 13 (MLP), Epoch 4, Train Loss: 1.0719, Valid Loss: 0.8138\n",
      "Model Num 13 (MLP), Epoch 5, Train Loss: 0.9836, Valid Loss: 0.8189\n",
      "Model Num 13 (MLP), Epoch 6, Train Loss: 0.9356, Valid Loss: 0.8130\n",
      "Model Num 13 (MLP), Epoch 7, Train Loss: 1.2517, Valid Loss: 0.8126\n",
      "Model Num 13 (MLP), Epoch 8, Train Loss: 1.2922, Valid Loss: 0.8114\n",
      "Model Num 13 (MLP), Epoch 9, Train Loss: 0.9235, Valid Loss: 0.8191\n",
      "Model Num 13 (MLP), Epoch 10, Train Loss: 1.2001, Valid Loss: 0.8232\n",
      "Model Num 13 (MLP), Epoch 11, Train Loss: 2.0195, Valid Loss: 0.8384\n",
      "Model Num 13 (MLP), Epoch 12, Train Loss: 3.5687, Valid Loss: 0.8186\n",
      "Model Num 13 (MLP), Epoch 13, Train Loss: 5.3662, Valid Loss: 0.8116\n",
      "Model Num 13 (MLP), Epoch 14, Train Loss: 1.3231, Valid Loss: 0.8144\n",
      "Model Num 13 (MLP), Epoch 15, Train Loss: 0.9441, Valid Loss: 0.8159\n",
      "Model Num 13 (MLP), Epoch 16, Train Loss: 0.9024, Valid Loss: 0.8174\n",
      "Model Num 13 (MLP), Epoch 17, Train Loss: 1.9594, Valid Loss: 0.8137\n",
      "Model Num 13 (MLP), Epoch 18, Train Loss: 1.1275, Valid Loss: 0.8151\n",
      "Early stopping for model 13 (MLP) at epoch 18!\n",
      "Loading best model for MLP_13 with validation loss: 0.8114\n",
      "------------------------------------------------------------\n",
      "Model Num 14 (MLP), Epoch 1, Train Loss: 1.0273, Valid Loss: 0.8259\n",
      "Model Num 14 (MLP), Epoch 2, Train Loss: 1.0089, Valid Loss: 0.8120\n",
      "Model Num 14 (MLP), Epoch 3, Train Loss: 0.9474, Valid Loss: 0.8120\n",
      "Model Num 14 (MLP), Epoch 4, Train Loss: 0.9239, Valid Loss: 0.8113\n",
      "Model Num 14 (MLP), Epoch 5, Train Loss: 0.9505, Valid Loss: 0.8145\n",
      "Model Num 14 (MLP), Epoch 6, Train Loss: 0.9405, Valid Loss: 0.8136\n",
      "Model Num 14 (MLP), Epoch 7, Train Loss: 0.9107, Valid Loss: 0.8176\n",
      "Model Num 14 (MLP), Epoch 8, Train Loss: 0.9024, Valid Loss: 0.8148\n",
      "Model Num 14 (MLP), Epoch 9, Train Loss: 1.3503, Valid Loss: 0.8151\n",
      "Model Num 14 (MLP), Epoch 10, Train Loss: 3.9887, Valid Loss: 0.8125\n",
      "Model Num 14 (MLP), Epoch 11, Train Loss: 1.1169, Valid Loss: 0.8228\n",
      "Model Num 14 (MLP), Epoch 12, Train Loss: 0.9169, Valid Loss: 0.8166\n",
      "Model Num 14 (MLP), Epoch 13, Train Loss: 0.8889, Valid Loss: 0.8216\n",
      "Model Num 14 (MLP), Epoch 14, Train Loss: 0.8819, Valid Loss: 0.8174\n",
      "Early stopping for model 14 (MLP) at epoch 14!\n",
      "Loading best model for MLP_14 with validation loss: 0.8113\n",
      "------------------------------------------------------------\n",
      "Model Num 15 (MLP), Epoch 1, Train Loss: 1.0500, Valid Loss: 0.8155\n",
      "Model Num 15 (MLP), Epoch 2, Train Loss: 1.0111, Valid Loss: 0.8179\n",
      "Model Num 15 (MLP), Epoch 3, Train Loss: 0.9917, Valid Loss: 0.8167\n",
      "Model Num 15 (MLP), Epoch 4, Train Loss: 0.9739, Valid Loss: 0.8134\n",
      "Model Num 15 (MLP), Epoch 5, Train Loss: 0.9378, Valid Loss: 0.8148\n",
      "Model Num 15 (MLP), Epoch 6, Train Loss: 0.8857, Valid Loss: 0.8259\n",
      "Model Num 15 (MLP), Epoch 7, Train Loss: 0.8986, Valid Loss: 0.8138\n",
      "Model Num 15 (MLP), Epoch 8, Train Loss: 0.8867, Valid Loss: 0.8211\n",
      "Model Num 15 (MLP), Epoch 9, Train Loss: 2.0287, Valid Loss: 0.8184\n",
      "Model Num 15 (MLP), Epoch 10, Train Loss: 1.3865, Valid Loss: 0.8164\n",
      "Model Num 15 (MLP), Epoch 11, Train Loss: 4.7620, Valid Loss: 0.8184\n",
      "Model Num 15 (MLP), Epoch 12, Train Loss: 4.3543, Valid Loss: 0.8138\n",
      "Model Num 15 (MLP), Epoch 13, Train Loss: 0.9203, Valid Loss: 0.8220\n",
      "Model Num 15 (MLP), Epoch 14, Train Loss: 1.0237, Valid Loss: 0.8199\n",
      "Early stopping for model 15 (MLP) at epoch 14!\n",
      "Loading best model for MLP_15 with validation loss: 0.8134\n",
      "------------------------------------------------------------\n",
      "Model Num 16 (MLP), Epoch 1, Train Loss: 0.9766, Valid Loss: 0.8339\n",
      "Model Num 16 (MLP), Epoch 2, Train Loss: 1.0141, Valid Loss: 0.8301\n",
      "Model Num 16 (MLP), Epoch 3, Train Loss: 0.9644, Valid Loss: 0.8204\n",
      "Model Num 16 (MLP), Epoch 4, Train Loss: 0.9119, Valid Loss: 0.8129\n",
      "Model Num 16 (MLP), Epoch 5, Train Loss: 0.8819, Valid Loss: 0.8167\n",
      "Model Num 16 (MLP), Epoch 6, Train Loss: 0.9514, Valid Loss: 0.8112\n",
      "Model Num 16 (MLP), Epoch 7, Train Loss: 1.0483, Valid Loss: 0.8147\n",
      "Model Num 16 (MLP), Epoch 8, Train Loss: 2.7514, Valid Loss: 0.8166\n",
      "Model Num 16 (MLP), Epoch 9, Train Loss: 1.2104, Valid Loss: 0.8182\n",
      "Model Num 16 (MLP), Epoch 10, Train Loss: 0.9472, Valid Loss: 0.8159\n",
      "Model Num 16 (MLP), Epoch 11, Train Loss: 0.9107, Valid Loss: 0.8194\n",
      "Model Num 16 (MLP), Epoch 12, Train Loss: 0.8858, Valid Loss: 0.8146\n",
      "Model Num 16 (MLP), Epoch 13, Train Loss: 0.8709, Valid Loss: 0.8155\n",
      "Model Num 16 (MLP), Epoch 14, Train Loss: 0.8572, Valid Loss: 0.8155\n",
      "Model Num 16 (MLP), Epoch 15, Train Loss: 0.8413, Valid Loss: 0.8150\n",
      "Model Num 16 (MLP), Epoch 16, Train Loss: 0.9147, Valid Loss: 0.8170\n",
      "Early stopping for model 16 (MLP) at epoch 16!\n",
      "Loading best model for MLP_16 with validation loss: 0.8112\n",
      "------------------------------------------------------------\n",
      "Model Num 17 (MLP), Epoch 1, Train Loss: 0.9403, Valid Loss: 0.8148\n",
      "Model Num 17 (MLP), Epoch 2, Train Loss: 0.9310, Valid Loss: 0.8262\n",
      "Model Num 17 (MLP), Epoch 3, Train Loss: 1.0176, Valid Loss: 0.8148\n",
      "Model Num 17 (MLP), Epoch 4, Train Loss: 0.8929, Valid Loss: 0.8128\n",
      "Model Num 17 (MLP), Epoch 5, Train Loss: 0.8695, Valid Loss: 0.8091\n",
      "Model Num 17 (MLP), Epoch 6, Train Loss: 0.8568, Valid Loss: 0.8091\n",
      "Model Num 17 (MLP), Epoch 7, Train Loss: 0.8370, Valid Loss: 0.8087\n",
      "Model Num 17 (MLP), Epoch 8, Train Loss: 0.8225, Valid Loss: 0.8108\n",
      "Model Num 17 (MLP), Epoch 9, Train Loss: 0.8223, Valid Loss: 0.8131\n",
      "Model Num 17 (MLP), Epoch 10, Train Loss: 0.8715, Valid Loss: 0.8092\n",
      "Model Num 17 (MLP), Epoch 11, Train Loss: 1.0705, Valid Loss: 0.8101\n",
      "Model Num 17 (MLP), Epoch 12, Train Loss: 1.1913, Valid Loss: 0.8099\n",
      "Model Num 17 (MLP), Epoch 13, Train Loss: 1.5116, Valid Loss: 0.8108\n",
      "Model Num 17 (MLP), Epoch 14, Train Loss: 1.3135, Valid Loss: 0.8223\n",
      "Model Num 17 (MLP), Epoch 15, Train Loss: 0.8913, Valid Loss: 0.8123\n",
      "Model Num 17 (MLP), Epoch 16, Train Loss: 0.8399, Valid Loss: 0.8124\n",
      "Model Num 17 (MLP), Epoch 17, Train Loss: 0.8368, Valid Loss: 0.8116\n",
      "Early stopping for model 17 (MLP) at epoch 17!\n",
      "Loading best model for MLP_17 with validation loss: 0.8087\n",
      "------------------------------------------------------------\n",
      "Model Num 18 (MLP), Epoch 1, Train Loss: 1.0211, Valid Loss: 0.8234\n",
      "Model Num 18 (MLP), Epoch 2, Train Loss: 0.9867, Valid Loss: 0.8210\n",
      "Model Num 18 (MLP), Epoch 3, Train Loss: 0.9993, Valid Loss: 0.8131\n",
      "Model Num 18 (MLP), Epoch 4, Train Loss: 0.9348, Valid Loss: 0.8106\n",
      "Model Num 18 (MLP), Epoch 5, Train Loss: 0.9577, Valid Loss: 0.8121\n",
      "Model Num 18 (MLP), Epoch 6, Train Loss: 0.9420, Valid Loss: 0.8096\n",
      "Model Num 18 (MLP), Epoch 7, Train Loss: 1.0300, Valid Loss: 0.8109\n",
      "Model Num 18 (MLP), Epoch 8, Train Loss: 1.0435, Valid Loss: 0.8229\n",
      "Model Num 18 (MLP), Epoch 9, Train Loss: 1.2249, Valid Loss: 0.8083\n",
      "Model Num 18 (MLP), Epoch 10, Train Loss: 0.9420, Valid Loss: 0.8088\n",
      "Model Num 18 (MLP), Epoch 11, Train Loss: 0.9723, Valid Loss: 0.8091\n",
      "Model Num 18 (MLP), Epoch 12, Train Loss: 1.1707, Valid Loss: 0.8296\n",
      "Model Num 18 (MLP), Epoch 13, Train Loss: 1.7139, Valid Loss: 0.8087\n",
      "Model Num 18 (MLP), Epoch 14, Train Loss: 0.9446, Valid Loss: 0.8152\n",
      "Model Num 18 (MLP), Epoch 15, Train Loss: 0.8736, Valid Loss: 0.8140\n",
      "Model Num 18 (MLP), Epoch 16, Train Loss: 0.8790, Valid Loss: 0.8082\n",
      "Model Num 18 (MLP), Epoch 17, Train Loss: 0.8569, Valid Loss: 0.8089\n",
      "Model Num 18 (MLP), Epoch 18, Train Loss: 0.8536, Valid Loss: 0.8105\n",
      "Model Num 18 (MLP), Epoch 19, Train Loss: 0.8571, Valid Loss: 0.8129\n",
      "Model Num 18 (MLP), Epoch 20, Train Loss: 0.9531, Valid Loss: 0.8124\n",
      "Loading best model for MLP_18 with validation loss: 0.8082\n",
      "------------------------------------------------------------\n",
      "Model Num 19 (MLP), Epoch 1, Train Loss: 1.0114, Valid Loss: 0.8250\n",
      "Model Num 19 (MLP), Epoch 2, Train Loss: 0.9439, Valid Loss: 0.8204\n",
      "Model Num 19 (MLP), Epoch 3, Train Loss: 0.9603, Valid Loss: 0.8139\n",
      "Model Num 19 (MLP), Epoch 4, Train Loss: 0.9440, Valid Loss: 0.8176\n",
      "Model Num 19 (MLP), Epoch 5, Train Loss: 0.9512, Valid Loss: 0.8141\n",
      "Model Num 19 (MLP), Epoch 6, Train Loss: 0.8913, Valid Loss: 0.8165\n",
      "Model Num 19 (MLP), Epoch 7, Train Loss: 0.8983, Valid Loss: 0.8147\n",
      "Model Num 19 (MLP), Epoch 8, Train Loss: 1.0323, Valid Loss: 0.8228\n",
      "Model Num 19 (MLP), Epoch 9, Train Loss: 1.6907, Valid Loss: 0.8154\n",
      "Model Num 19 (MLP), Epoch 10, Train Loss: 1.3074, Valid Loss: 0.8129\n",
      "Model Num 19 (MLP), Epoch 11, Train Loss: 0.9497, Valid Loss: 0.8144\n",
      "Model Num 19 (MLP), Epoch 12, Train Loss: 0.9368, Valid Loss: 0.8122\n",
      "Model Num 19 (MLP), Epoch 13, Train Loss: 0.8606, Valid Loss: 0.8128\n",
      "Model Num 19 (MLP), Epoch 14, Train Loss: 0.8425, Valid Loss: 0.8136\n",
      "Model Num 19 (MLP), Epoch 15, Train Loss: 0.8713, Valid Loss: 0.8127\n",
      "Model Num 19 (MLP), Epoch 16, Train Loss: 5.4041, Valid Loss: 0.8138\n",
      "Model Num 19 (MLP), Epoch 17, Train Loss: 1.8640, Valid Loss: 0.8209\n",
      "Model Num 19 (MLP), Epoch 18, Train Loss: 1.0821, Valid Loss: 0.8147\n",
      "Model Num 19 (MLP), Epoch 19, Train Loss: 0.9078, Valid Loss: 0.8189\n",
      "Model Num 19 (MLP), Epoch 20, Train Loss: 0.8837, Valid Loss: 0.8121\n",
      "Loading best model for MLP_19 with validation loss: 0.8121\n",
      "------------------------------------------------------------\n",
      "Model Num 20 (MLP), Epoch 1, Train Loss: 0.9729, Valid Loss: 0.8129\n",
      "Model Num 20 (MLP), Epoch 2, Train Loss: 0.9600, Valid Loss: 0.8135\n",
      "Model Num 20 (MLP), Epoch 3, Train Loss: 0.9194, Valid Loss: 0.8172\n",
      "Model Num 20 (MLP), Epoch 4, Train Loss: 0.9288, Valid Loss: 0.8144\n",
      "Model Num 20 (MLP), Epoch 5, Train Loss: 0.8983, Valid Loss: 0.8177\n",
      "Model Num 20 (MLP), Epoch 6, Train Loss: 0.8969, Valid Loss: 0.8268\n",
      "Model Num 20 (MLP), Epoch 7, Train Loss: 0.9076, Valid Loss: 0.8083\n",
      "Model Num 20 (MLP), Epoch 8, Train Loss: 0.8796, Valid Loss: 0.8147\n",
      "Model Num 20 (MLP), Epoch 9, Train Loss: 0.9512, Valid Loss: 0.8110\n",
      "Model Num 20 (MLP), Epoch 10, Train Loss: 0.8798, Valid Loss: 0.8119\n",
      "Model Num 20 (MLP), Epoch 11, Train Loss: 0.8980, Valid Loss: 0.8171\n",
      "Model Num 20 (MLP), Epoch 12, Train Loss: 0.9075, Valid Loss: 0.8160\n",
      "Model Num 20 (MLP), Epoch 13, Train Loss: 1.5566, Valid Loss: 0.8193\n",
      "Model Num 20 (MLP), Epoch 14, Train Loss: 3.0422, Valid Loss: 0.8247\n",
      "Model Num 20 (MLP), Epoch 15, Train Loss: 3.0813, Valid Loss: 0.8151\n",
      "Model Num 20 (MLP), Epoch 16, Train Loss: 14.4774, Valid Loss: 0.8209\n",
      "Model Num 20 (MLP), Epoch 17, Train Loss: 4.8467, Valid Loss: 0.8163\n",
      "Early stopping for model 20 (MLP) at epoch 17!\n",
      "Loading best model for MLP_20 with validation loss: 0.8083\n",
      "------------------------------------------------------------\n",
      "Model Num 21 (MLP), Epoch 1, Train Loss: 1.0444, Valid Loss: 0.8301\n",
      "Model Num 21 (MLP), Epoch 2, Train Loss: 1.0089, Valid Loss: 0.8208\n",
      "Model Num 21 (MLP), Epoch 3, Train Loss: 0.9655, Valid Loss: 0.8135\n",
      "Model Num 21 (MLP), Epoch 4, Train Loss: 0.9566, Valid Loss: 0.8155\n",
      "Model Num 21 (MLP), Epoch 5, Train Loss: 0.9662, Valid Loss: 0.8162\n",
      "Model Num 21 (MLP), Epoch 6, Train Loss: 0.9667, Valid Loss: 0.8113\n",
      "Model Num 21 (MLP), Epoch 7, Train Loss: 1.0819, Valid Loss: 0.8241\n",
      "Model Num 21 (MLP), Epoch 8, Train Loss: 2.2255, Valid Loss: 0.8141\n",
      "Model Num 21 (MLP), Epoch 9, Train Loss: 3.8521, Valid Loss: 0.8118\n",
      "Model Num 21 (MLP), Epoch 10, Train Loss: 1.0913, Valid Loss: 0.8113\n",
      "Model Num 21 (MLP), Epoch 11, Train Loss: 0.9506, Valid Loss: 0.8119\n",
      "Model Num 21 (MLP), Epoch 12, Train Loss: 0.9261, Valid Loss: 0.8100\n",
      "Model Num 21 (MLP), Epoch 13, Train Loss: 0.9550, Valid Loss: 0.8094\n",
      "Model Num 21 (MLP), Epoch 14, Train Loss: 0.9072, Valid Loss: 0.8100\n",
      "Model Num 21 (MLP), Epoch 15, Train Loss: 0.9048, Valid Loss: 0.8102\n",
      "Model Num 21 (MLP), Epoch 16, Train Loss: 0.9140, Valid Loss: 0.8116\n",
      "Model Num 21 (MLP), Epoch 17, Train Loss: 0.9163, Valid Loss: 0.8097\n",
      "Model Num 21 (MLP), Epoch 18, Train Loss: 0.9498, Valid Loss: 0.8131\n",
      "Model Num 21 (MLP), Epoch 19, Train Loss: 1.1157, Valid Loss: 0.8118\n",
      "Model Num 21 (MLP), Epoch 20, Train Loss: 0.9808, Valid Loss: 0.8099\n",
      "Loading best model for MLP_21 with validation loss: 0.8094\n",
      "------------------------------------------------------------\n",
      "Model Num 22 (MLP), Epoch 1, Train Loss: 1.0192, Valid Loss: 0.8141\n",
      "Model Num 22 (MLP), Epoch 2, Train Loss: 0.9699, Valid Loss: 0.8195\n",
      "Model Num 22 (MLP), Epoch 3, Train Loss: 0.9529, Valid Loss: 0.8191\n",
      "Model Num 22 (MLP), Epoch 4, Train Loss: 1.2120, Valid Loss: 0.8197\n",
      "Model Num 22 (MLP), Epoch 5, Train Loss: 1.1098, Valid Loss: 0.8131\n",
      "Model Num 22 (MLP), Epoch 6, Train Loss: 0.9553, Valid Loss: 0.8144\n",
      "Model Num 22 (MLP), Epoch 7, Train Loss: 1.8121, Valid Loss: 0.8167\n",
      "Model Num 22 (MLP), Epoch 8, Train Loss: 3.4371, Valid Loss: 0.8141\n",
      "Model Num 22 (MLP), Epoch 9, Train Loss: 5.8254, Valid Loss: 0.8125\n",
      "Model Num 22 (MLP), Epoch 10, Train Loss: 1.0992, Valid Loss: 0.8180\n",
      "Model Num 22 (MLP), Epoch 11, Train Loss: 2.3941, Valid Loss: 0.8107\n",
      "Model Num 22 (MLP), Epoch 12, Train Loss: 1.0991, Valid Loss: 0.8157\n",
      "Model Num 22 (MLP), Epoch 13, Train Loss: 1.2485, Valid Loss: 0.8106\n",
      "Model Num 22 (MLP), Epoch 14, Train Loss: 1.4477, Valid Loss: 0.8112\n",
      "Model Num 22 (MLP), Epoch 15, Train Loss: 0.9423, Valid Loss: 0.8143\n",
      "Model Num 22 (MLP), Epoch 16, Train Loss: 1.3771, Valid Loss: 0.8483\n",
      "Model Num 22 (MLP), Epoch 17, Train Loss: 1.1392, Valid Loss: 0.8127\n",
      "Model Num 22 (MLP), Epoch 18, Train Loss: 1.6742, Valid Loss: 0.8102\n",
      "Model Num 22 (MLP), Epoch 19, Train Loss: 0.9926, Valid Loss: 0.8131\n",
      "Model Num 22 (MLP), Epoch 20, Train Loss: 1.5735, Valid Loss: 0.8135\n",
      "Loading best model for MLP_22 with validation loss: 0.8102\n",
      "------------------------------------------------------------\n",
      "Model Num 23 (MLP), Epoch 1, Train Loss: 0.9747, Valid Loss: 0.8162\n",
      "Model Num 23 (MLP), Epoch 2, Train Loss: 0.9364, Valid Loss: 0.8133\n",
      "Model Num 23 (MLP), Epoch 3, Train Loss: 0.9317, Valid Loss: 0.8448\n",
      "Model Num 23 (MLP), Epoch 4, Train Loss: 0.9168, Valid Loss: 0.8133\n",
      "Model Num 23 (MLP), Epoch 5, Train Loss: 0.9005, Valid Loss: 0.8213\n",
      "Model Num 23 (MLP), Epoch 6, Train Loss: 0.9261, Valid Loss: 0.8119\n",
      "Model Num 23 (MLP), Epoch 7, Train Loss: 0.8881, Valid Loss: 0.8106\n",
      "Model Num 23 (MLP), Epoch 8, Train Loss: 0.8691, Valid Loss: 0.8110\n",
      "Model Num 23 (MLP), Epoch 9, Train Loss: 0.9861, Valid Loss: 0.8222\n",
      "Model Num 23 (MLP), Epoch 10, Train Loss: 1.1748, Valid Loss: 0.8111\n",
      "Model Num 23 (MLP), Epoch 11, Train Loss: 1.1432, Valid Loss: 0.8121\n",
      "Model Num 23 (MLP), Epoch 12, Train Loss: 3.4165, Valid Loss: 0.8129\n",
      "Model Num 23 (MLP), Epoch 13, Train Loss: 3.3046, Valid Loss: 0.8193\n",
      "Model Num 23 (MLP), Epoch 14, Train Loss: 3.7065, Valid Loss: 0.8099\n",
      "Model Num 23 (MLP), Epoch 15, Train Loss: 1.5071, Valid Loss: 0.8107\n",
      "Model Num 23 (MLP), Epoch 16, Train Loss: 1.0801, Valid Loss: 0.8103\n",
      "Model Num 23 (MLP), Epoch 17, Train Loss: 0.8483, Valid Loss: 0.8094\n",
      "Model Num 23 (MLP), Epoch 18, Train Loss: 0.8873, Valid Loss: 0.8125\n",
      "Model Num 23 (MLP), Epoch 19, Train Loss: 0.9483, Valid Loss: 0.8110\n",
      "Model Num 23 (MLP), Epoch 20, Train Loss: 1.6970, Valid Loss: 0.8117\n",
      "Loading best model for MLP_23 with validation loss: 0.8094\n",
      "------------------------------------------------------------\n",
      "Model Num 24 (MLP), Epoch 1, Train Loss: 0.9962, Valid Loss: 0.8155\n",
      "Model Num 24 (MLP), Epoch 2, Train Loss: 0.9482, Valid Loss: 0.8258\n",
      "Model Num 24 (MLP), Epoch 3, Train Loss: 0.9558, Valid Loss: 0.8163\n",
      "Model Num 24 (MLP), Epoch 4, Train Loss: 0.8944, Valid Loss: 0.8168\n",
      "Model Num 24 (MLP), Epoch 5, Train Loss: 0.9353, Valid Loss: 0.8136\n",
      "Model Num 24 (MLP), Epoch 6, Train Loss: 0.9210, Valid Loss: 0.8096\n",
      "Model Num 24 (MLP), Epoch 7, Train Loss: 0.9362, Valid Loss: 0.8100\n",
      "Model Num 24 (MLP), Epoch 8, Train Loss: 0.9083, Valid Loss: 0.8115\n",
      "Model Num 24 (MLP), Epoch 9, Train Loss: 1.1399, Valid Loss: 0.8091\n",
      "Model Num 24 (MLP), Epoch 10, Train Loss: 1.3972, Valid Loss: 0.8193\n",
      "Model Num 24 (MLP), Epoch 11, Train Loss: 0.9032, Valid Loss: 0.8119\n",
      "Model Num 24 (MLP), Epoch 12, Train Loss: 0.9141, Valid Loss: 0.8113\n",
      "Model Num 24 (MLP), Epoch 13, Train Loss: 1.3299, Valid Loss: 0.8112\n",
      "Model Num 24 (MLP), Epoch 14, Train Loss: 1.3367, Valid Loss: 0.8123\n",
      "Model Num 24 (MLP), Epoch 15, Train Loss: 0.8759, Valid Loss: 0.8097\n",
      "Model Num 24 (MLP), Epoch 16, Train Loss: 0.9209, Valid Loss: 0.8105\n",
      "Model Num 24 (MLP), Epoch 17, Train Loss: 0.8849, Valid Loss: 0.8114\n",
      "Model Num 24 (MLP), Epoch 18, Train Loss: 0.9304, Valid Loss: 0.8136\n",
      "Model Num 24 (MLP), Epoch 19, Train Loss: 1.0565, Valid Loss: 0.8155\n",
      "Early stopping for model 24 (MLP) at epoch 19!\n",
      "Loading best model for MLP_24 with validation loss: 0.8091\n",
      "------------------------------------------------------------\n",
      "Model Num 25 (MLP), Epoch 1, Train Loss: 0.9996, Valid Loss: 0.8355\n",
      "Model Num 25 (MLP), Epoch 2, Train Loss: 0.9972, Valid Loss: 0.8142\n",
      "Model Num 25 (MLP), Epoch 3, Train Loss: 0.9795, Valid Loss: 0.8143\n",
      "Model Num 25 (MLP), Epoch 4, Train Loss: 0.9672, Valid Loss: 0.8147\n",
      "Model Num 25 (MLP), Epoch 5, Train Loss: 0.9868, Valid Loss: 0.8085\n",
      "Model Num 25 (MLP), Epoch 6, Train Loss: 0.9847, Valid Loss: 0.8108\n",
      "Model Num 25 (MLP), Epoch 7, Train Loss: 1.0113, Valid Loss: 0.8126\n",
      "Model Num 25 (MLP), Epoch 8, Train Loss: 1.1918, Valid Loss: 0.8159\n",
      "Model Num 25 (MLP), Epoch 9, Train Loss: 2.6473, Valid Loss: 0.8090\n",
      "Model Num 25 (MLP), Epoch 10, Train Loss: 2.0952, Valid Loss: 0.8110\n",
      "Model Num 25 (MLP), Epoch 11, Train Loss: 1.1922, Valid Loss: 0.8094\n",
      "Model Num 25 (MLP), Epoch 12, Train Loss: 2.2023, Valid Loss: 0.8118\n",
      "Model Num 25 (MLP), Epoch 13, Train Loss: 1.9386, Valid Loss: 0.8092\n",
      "Model Num 25 (MLP), Epoch 14, Train Loss: 0.9465, Valid Loss: 0.8090\n",
      "Model Num 25 (MLP), Epoch 15, Train Loss: 0.9284, Valid Loss: 0.8092\n",
      "Early stopping for model 25 (MLP) at epoch 15!\n",
      "Loading best model for MLP_25 with validation loss: 0.8085\n",
      "------------------------------------------------------------\n",
      "Model Num 26 (MLP), Epoch 1, Train Loss: 0.9560, Valid Loss: 0.8279\n",
      "Model Num 26 (MLP), Epoch 2, Train Loss: 0.9277, Valid Loss: 0.8179\n",
      "Model Num 26 (MLP), Epoch 3, Train Loss: 0.9317, Valid Loss: 0.8128\n",
      "Model Num 26 (MLP), Epoch 4, Train Loss: 0.9034, Valid Loss: 0.8116\n",
      "Model Num 26 (MLP), Epoch 5, Train Loss: 0.8608, Valid Loss: 0.8162\n",
      "Model Num 26 (MLP), Epoch 6, Train Loss: 0.8795, Valid Loss: 0.8114\n",
      "Model Num 26 (MLP), Epoch 7, Train Loss: 0.9012, Valid Loss: 0.8148\n",
      "Model Num 26 (MLP), Epoch 8, Train Loss: 1.4905, Valid Loss: 0.8096\n",
      "Model Num 26 (MLP), Epoch 9, Train Loss: 1.9136, Valid Loss: 0.8201\n",
      "Model Num 26 (MLP), Epoch 10, Train Loss: 11.8701, Valid Loss: 0.8116\n",
      "Model Num 26 (MLP), Epoch 11, Train Loss: 2.6916, Valid Loss: 0.8091\n",
      "Model Num 26 (MLP), Epoch 12, Train Loss: 0.8743, Valid Loss: 0.8093\n",
      "Model Num 26 (MLP), Epoch 13, Train Loss: 0.8674, Valid Loss: 0.8090\n",
      "Model Num 26 (MLP), Epoch 14, Train Loss: 0.8548, Valid Loss: 0.8095\n",
      "Model Num 26 (MLP), Epoch 15, Train Loss: 0.8439, Valid Loss: 0.8102\n",
      "Model Num 26 (MLP), Epoch 16, Train Loss: 0.8458, Valid Loss: 0.8100\n",
      "Model Num 26 (MLP), Epoch 17, Train Loss: 0.8323, Valid Loss: 0.8088\n",
      "Model Num 26 (MLP), Epoch 18, Train Loss: 0.8472, Valid Loss: 0.8101\n",
      "Model Num 26 (MLP), Epoch 19, Train Loss: 0.8762, Valid Loss: 0.8114\n",
      "Model Num 26 (MLP), Epoch 20, Train Loss: 0.8504, Valid Loss: 0.8111\n",
      "Loading best model for MLP_26 with validation loss: 0.8088\n",
      "------------------------------------------------------------\n",
      "Model Num 27 (MLP), Epoch 1, Train Loss: 0.9722, Valid Loss: 0.8282\n",
      "Model Num 27 (MLP), Epoch 2, Train Loss: 0.9373, Valid Loss: 0.8195\n",
      "Model Num 27 (MLP), Epoch 3, Train Loss: 0.9123, Valid Loss: 0.8191\n",
      "Model Num 27 (MLP), Epoch 4, Train Loss: 0.9605, Valid Loss: 0.8136\n",
      "Model Num 27 (MLP), Epoch 5, Train Loss: 0.8830, Valid Loss: 0.8106\n",
      "Model Num 27 (MLP), Epoch 6, Train Loss: 0.8683, Valid Loss: 0.8099\n",
      "Model Num 27 (MLP), Epoch 7, Train Loss: 0.8293, Valid Loss: 0.8127\n",
      "Model Num 27 (MLP), Epoch 8, Train Loss: 0.8355, Valid Loss: 0.8155\n",
      "Model Num 27 (MLP), Epoch 9, Train Loss: 1.0783, Valid Loss: 0.8150\n",
      "Model Num 27 (MLP), Epoch 10, Train Loss: 24.0708, Valid Loss: 0.8140\n",
      "Model Num 27 (MLP), Epoch 11, Train Loss: 4.6078, Valid Loss: 0.8283\n",
      "Model Num 27 (MLP), Epoch 12, Train Loss: 8.8157, Valid Loss: 0.8255\n",
      "Model Num 27 (MLP), Epoch 13, Train Loss: 4.1183, Valid Loss: 0.8103\n",
      "Model Num 27 (MLP), Epoch 14, Train Loss: 0.8652, Valid Loss: 0.8115\n",
      "Model Num 27 (MLP), Epoch 15, Train Loss: 0.8728, Valid Loss: 0.8120\n",
      "Model Num 27 (MLP), Epoch 16, Train Loss: 0.8766, Valid Loss: 0.8103\n",
      "Early stopping for model 27 (MLP) at epoch 16!\n",
      "Loading best model for MLP_27 with validation loss: 0.8099\n",
      "------------------------------------------------------------\n",
      "Model Num 28 (MLP), Epoch 1, Train Loss: 0.9677, Valid Loss: 0.8253\n",
      "Model Num 28 (MLP), Epoch 2, Train Loss: 0.9259, Valid Loss: 0.8126\n",
      "Model Num 28 (MLP), Epoch 3, Train Loss: 0.9117, Valid Loss: 0.8159\n",
      "Model Num 28 (MLP), Epoch 4, Train Loss: 0.8825, Valid Loss: 0.8138\n",
      "Model Num 28 (MLP), Epoch 5, Train Loss: 0.8948, Valid Loss: 0.8115\n",
      "Model Num 28 (MLP), Epoch 6, Train Loss: 0.9309, Valid Loss: 0.8251\n",
      "Model Num 28 (MLP), Epoch 7, Train Loss: 1.0537, Valid Loss: 0.8118\n",
      "Model Num 28 (MLP), Epoch 8, Train Loss: 2.4087, Valid Loss: 0.8175\n",
      "Model Num 28 (MLP), Epoch 9, Train Loss: 1.0375, Valid Loss: 0.8142\n",
      "Model Num 28 (MLP), Epoch 10, Train Loss: 0.8851, Valid Loss: 0.8096\n",
      "Model Num 28 (MLP), Epoch 11, Train Loss: 0.9132, Valid Loss: 0.8119\n",
      "Model Num 28 (MLP), Epoch 12, Train Loss: 0.9054, Valid Loss: 0.8087\n",
      "Model Num 28 (MLP), Epoch 13, Train Loss: 0.9463, Valid Loss: 0.8087\n",
      "Model Num 28 (MLP), Epoch 14, Train Loss: 3.8549, Valid Loss: 0.8094\n",
      "Model Num 28 (MLP), Epoch 15, Train Loss: 4.6466, Valid Loss: 0.8232\n",
      "Model Num 28 (MLP), Epoch 16, Train Loss: 1.6361, Valid Loss: 0.8094\n",
      "Model Num 28 (MLP), Epoch 17, Train Loss: 4.0304, Valid Loss: 0.8119\n",
      "Model Num 28 (MLP), Epoch 18, Train Loss: 0.9025, Valid Loss: 0.8097\n",
      "Model Num 28 (MLP), Epoch 19, Train Loss: 0.9184, Valid Loss: 0.8098\n",
      "Model Num 28 (MLP), Epoch 20, Train Loss: 0.9094, Valid Loss: 0.8095\n",
      "Loading best model for MLP_28 with validation loss: 0.8087\n",
      "------------------------------------------------------------\n",
      "Model Num 29 (MLP), Epoch 1, Train Loss: 0.9822, Valid Loss: 0.8161\n",
      "Model Num 29 (MLP), Epoch 2, Train Loss: 0.9563, Valid Loss: 0.8200\n",
      "Model Num 29 (MLP), Epoch 3, Train Loss: 0.9594, Valid Loss: 0.8138\n",
      "Model Num 29 (MLP), Epoch 4, Train Loss: 0.9562, Valid Loss: 0.8113\n",
      "Model Num 29 (MLP), Epoch 5, Train Loss: 1.0823, Valid Loss: 0.8115\n",
      "Model Num 29 (MLP), Epoch 6, Train Loss: 0.9556, Valid Loss: 0.8135\n",
      "Model Num 29 (MLP), Epoch 7, Train Loss: 0.9043, Valid Loss: 0.8142\n",
      "Model Num 29 (MLP), Epoch 8, Train Loss: 0.9084, Valid Loss: 0.8150\n",
      "Model Num 29 (MLP), Epoch 9, Train Loss: 0.9900, Valid Loss: 0.8116\n",
      "Model Num 29 (MLP), Epoch 10, Train Loss: 22.7393, Valid Loss: 0.8154\n",
      "Model Num 29 (MLP), Epoch 11, Train Loss: 2.1083, Valid Loss: 0.8136\n",
      "Model Num 29 (MLP), Epoch 12, Train Loss: 0.9510, Valid Loss: 0.8121\n",
      "Model Num 29 (MLP), Epoch 13, Train Loss: 0.8897, Valid Loss: 0.8106\n",
      "Model Num 29 (MLP), Epoch 14, Train Loss: 0.9221, Valid Loss: 0.8134\n",
      "Model Num 29 (MLP), Epoch 15, Train Loss: 0.8818, Valid Loss: 0.8144\n",
      "Model Num 29 (MLP), Epoch 16, Train Loss: 0.8756, Valid Loss: 0.8129\n",
      "Model Num 29 (MLP), Epoch 17, Train Loss: 0.8676, Valid Loss: 0.8183\n",
      "Model Num 29 (MLP), Epoch 18, Train Loss: 0.9837, Valid Loss: 0.8150\n",
      "Model Num 29 (MLP), Epoch 19, Train Loss: 0.9033, Valid Loss: 0.8138\n",
      "Model Num 29 (MLP), Epoch 20, Train Loss: 0.9379, Valid Loss: 0.8127\n",
      "Loading best model for MLP_29 with validation loss: 0.8106\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models, indices_bootstrap = train(\n",
    "    model_cls=MLP,\n",
    "    data_loader=bootstrap_loaders,\n",
    "    valid_data_loader=valid_loader,\n",
    "    EPOCHS = 20,\n",
    "    valid_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9132080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Num 0 (DLinear), Epoch 1, Train Loss: 0.9875, Valid Loss: 0.8160\n",
      "Model Num 0 (DLinear), Epoch 2, Train Loss: 0.9649, Valid Loss: 0.8204\n",
      "Model Num 0 (DLinear), Epoch 3, Train Loss: 0.9636, Valid Loss: 0.8239\n",
      "Model Num 0 (DLinear), Epoch 4, Train Loss: 0.9627, Valid Loss: 0.8234\n",
      "Model Num 0 (DLinear), Epoch 5, Train Loss: 0.9660, Valid Loss: 0.8253\n",
      "Model Num 0 (DLinear), Epoch 6, Train Loss: 0.9626, Valid Loss: 0.8251\n",
      "Model Num 0 (DLinear), Epoch 7, Train Loss: 0.9632, Valid Loss: 0.8259\n",
      "Model Num 0 (DLinear), Epoch 8, Train Loss: 0.9640, Valid Loss: 0.8270\n",
      "Model Num 0 (DLinear), Epoch 9, Train Loss: 0.9634, Valid Loss: 0.8254\n",
      "Model Num 0 (DLinear), Epoch 10, Train Loss: 0.9630, Valid Loss: 0.8266\n",
      "Model Num 0 (DLinear), Epoch 11, Train Loss: 0.9638, Valid Loss: 0.8258\n",
      "Early stopping for model 0 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_0 with validation loss: 0.8160\n",
      "------------------------------------------------------------\n",
      "Model Num 1 (DLinear), Epoch 1, Train Loss: 0.9863, Valid Loss: 0.8169\n",
      "Model Num 1 (DLinear), Epoch 2, Train Loss: 0.9724, Valid Loss: 0.8214\n",
      "Model Num 1 (DLinear), Epoch 3, Train Loss: 0.9702, Valid Loss: 0.8254\n",
      "Model Num 1 (DLinear), Epoch 4, Train Loss: 0.9705, Valid Loss: 0.8246\n",
      "Model Num 1 (DLinear), Epoch 5, Train Loss: 0.9703, Valid Loss: 0.8277\n",
      "Model Num 1 (DLinear), Epoch 6, Train Loss: 0.9806, Valid Loss: 0.8258\n",
      "Model Num 1 (DLinear), Epoch 7, Train Loss: 0.9705, Valid Loss: 0.8258\n",
      "Model Num 1 (DLinear), Epoch 8, Train Loss: 0.9697, Valid Loss: 0.8294\n",
      "Model Num 1 (DLinear), Epoch 9, Train Loss: 0.9707, Valid Loss: 0.8268\n",
      "Model Num 1 (DLinear), Epoch 10, Train Loss: 0.9720, Valid Loss: 0.8278\n",
      "Model Num 1 (DLinear), Epoch 11, Train Loss: 0.9698, Valid Loss: 0.8271\n",
      "Early stopping for model 1 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_1 with validation loss: 0.8169\n",
      "------------------------------------------------------------\n",
      "Model Num 2 (DLinear), Epoch 1, Train Loss: 1.0287, Valid Loss: 0.8155\n",
      "Model Num 2 (DLinear), Epoch 2, Train Loss: 1.0124, Valid Loss: 0.8214\n",
      "Model Num 2 (DLinear), Epoch 3, Train Loss: 1.0104, Valid Loss: 0.8259\n",
      "Model Num 2 (DLinear), Epoch 4, Train Loss: 1.0084, Valid Loss: 0.8277\n",
      "Model Num 2 (DLinear), Epoch 5, Train Loss: 1.0119, Valid Loss: 0.8271\n",
      "Model Num 2 (DLinear), Epoch 6, Train Loss: 1.0088, Valid Loss: 0.8285\n",
      "Model Num 2 (DLinear), Epoch 7, Train Loss: 1.0097, Valid Loss: 0.8277\n",
      "Model Num 2 (DLinear), Epoch 8, Train Loss: 1.0110, Valid Loss: 0.8296\n",
      "Model Num 2 (DLinear), Epoch 9, Train Loss: 1.0087, Valid Loss: 0.8288\n",
      "Model Num 2 (DLinear), Epoch 10, Train Loss: 1.0090, Valid Loss: 0.8291\n",
      "Model Num 2 (DLinear), Epoch 11, Train Loss: 1.0136, Valid Loss: 0.8287\n",
      "Early stopping for model 2 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_2 with validation loss: 0.8155\n",
      "------------------------------------------------------------\n",
      "Model Num 3 (DLinear), Epoch 1, Train Loss: 1.0377, Valid Loss: 0.8151\n",
      "Model Num 3 (DLinear), Epoch 2, Train Loss: 1.0305, Valid Loss: 0.8208\n",
      "Model Num 3 (DLinear), Epoch 3, Train Loss: 1.0214, Valid Loss: 0.8260\n",
      "Model Num 3 (DLinear), Epoch 4, Train Loss: 1.0187, Valid Loss: 0.8251\n",
      "Model Num 3 (DLinear), Epoch 5, Train Loss: 1.0185, Valid Loss: 0.8276\n",
      "Model Num 3 (DLinear), Epoch 6, Train Loss: 1.0183, Valid Loss: 0.8276\n",
      "Model Num 3 (DLinear), Epoch 7, Train Loss: 1.0221, Valid Loss: 0.8289\n",
      "Model Num 3 (DLinear), Epoch 8, Train Loss: 1.0188, Valid Loss: 0.8271\n",
      "Model Num 3 (DLinear), Epoch 9, Train Loss: 1.0207, Valid Loss: 0.8283\n",
      "Model Num 3 (DLinear), Epoch 10, Train Loss: 1.0204, Valid Loss: 0.8278\n",
      "Model Num 3 (DLinear), Epoch 11, Train Loss: 1.0197, Valid Loss: 0.8272\n",
      "Early stopping for model 3 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_3 with validation loss: 0.8151\n",
      "------------------------------------------------------------\n",
      "Model Num 4 (DLinear), Epoch 1, Train Loss: 1.0625, Valid Loss: 0.8162\n",
      "Model Num 4 (DLinear), Epoch 2, Train Loss: 1.0441, Valid Loss: 0.8233\n",
      "Model Num 4 (DLinear), Epoch 3, Train Loss: 1.0413, Valid Loss: 0.8260\n",
      "Model Num 4 (DLinear), Epoch 4, Train Loss: 1.0400, Valid Loss: 0.8302\n",
      "Model Num 4 (DLinear), Epoch 5, Train Loss: 1.0420, Valid Loss: 0.8294\n",
      "Model Num 4 (DLinear), Epoch 6, Train Loss: 1.0413, Valid Loss: 0.8308\n",
      "Model Num 4 (DLinear), Epoch 7, Train Loss: 1.0425, Valid Loss: 0.8302\n",
      "Model Num 4 (DLinear), Epoch 8, Train Loss: 1.0412, Valid Loss: 0.8294\n",
      "Model Num 4 (DLinear), Epoch 9, Train Loss: 1.0429, Valid Loss: 0.8295\n",
      "Model Num 4 (DLinear), Epoch 10, Train Loss: 1.0408, Valid Loss: 0.8306\n",
      "Model Num 4 (DLinear), Epoch 11, Train Loss: 1.0425, Valid Loss: 0.8325\n",
      "Early stopping for model 4 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_4 with validation loss: 0.8162\n",
      "------------------------------------------------------------\n",
      "Model Num 5 (DLinear), Epoch 1, Train Loss: 1.0055, Valid Loss: 0.8133\n",
      "Model Num 5 (DLinear), Epoch 2, Train Loss: 0.9902, Valid Loss: 0.8197\n",
      "Model Num 5 (DLinear), Epoch 3, Train Loss: 0.9866, Valid Loss: 0.8238\n",
      "Model Num 5 (DLinear), Epoch 4, Train Loss: 0.9866, Valid Loss: 0.8245\n",
      "Model Num 5 (DLinear), Epoch 5, Train Loss: 0.9851, Valid Loss: 0.8255\n",
      "Model Num 5 (DLinear), Epoch 6, Train Loss: 0.9853, Valid Loss: 0.8264\n",
      "Model Num 5 (DLinear), Epoch 7, Train Loss: 0.9854, Valid Loss: 0.8254\n",
      "Model Num 5 (DLinear), Epoch 8, Train Loss: 0.9851, Valid Loss: 0.8257\n",
      "Model Num 5 (DLinear), Epoch 9, Train Loss: 0.9851, Valid Loss: 0.8254\n",
      "Model Num 5 (DLinear), Epoch 10, Train Loss: 0.9864, Valid Loss: 0.8268\n",
      "Model Num 5 (DLinear), Epoch 11, Train Loss: 0.9862, Valid Loss: 0.8258\n",
      "Early stopping for model 5 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_5 with validation loss: 0.8133\n",
      "------------------------------------------------------------\n",
      "Model Num 6 (DLinear), Epoch 1, Train Loss: 1.0359, Valid Loss: 0.8137\n",
      "Model Num 6 (DLinear), Epoch 2, Train Loss: 1.0202, Valid Loss: 0.8202\n",
      "Model Num 6 (DLinear), Epoch 3, Train Loss: 1.0239, Valid Loss: 0.8229\n",
      "Model Num 6 (DLinear), Epoch 4, Train Loss: 1.0180, Valid Loss: 0.8255\n",
      "Model Num 6 (DLinear), Epoch 5, Train Loss: 1.0184, Valid Loss: 0.8250\n",
      "Model Num 6 (DLinear), Epoch 6, Train Loss: 1.0186, Valid Loss: 0.8235\n",
      "Model Num 6 (DLinear), Epoch 7, Train Loss: 1.0186, Valid Loss: 0.8254\n",
      "Model Num 6 (DLinear), Epoch 8, Train Loss: 1.0176, Valid Loss: 0.8257\n",
      "Model Num 6 (DLinear), Epoch 9, Train Loss: 1.0180, Valid Loss: 0.8256\n",
      "Model Num 6 (DLinear), Epoch 10, Train Loss: 1.0178, Valid Loss: 0.8250\n",
      "Model Num 6 (DLinear), Epoch 11, Train Loss: 1.0180, Valid Loss: 0.8257\n",
      "Early stopping for model 6 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_6 with validation loss: 0.8137\n",
      "------------------------------------------------------------\n",
      "Model Num 7 (DLinear), Epoch 1, Train Loss: 1.0158, Valid Loss: 0.8155\n",
      "Model Num 7 (DLinear), Epoch 2, Train Loss: 0.9948, Valid Loss: 0.8216\n",
      "Model Num 7 (DLinear), Epoch 3, Train Loss: 0.9965, Valid Loss: 0.8230\n",
      "Model Num 7 (DLinear), Epoch 4, Train Loss: 0.9924, Valid Loss: 0.8264\n",
      "Model Num 7 (DLinear), Epoch 5, Train Loss: 0.9930, Valid Loss: 0.8269\n",
      "Model Num 7 (DLinear), Epoch 6, Train Loss: 0.9926, Valid Loss: 0.8255\n",
      "Model Num 7 (DLinear), Epoch 7, Train Loss: 0.9927, Valid Loss: 0.8262\n",
      "Model Num 7 (DLinear), Epoch 8, Train Loss: 0.9969, Valid Loss: 0.8274\n",
      "Model Num 7 (DLinear), Epoch 9, Train Loss: 0.9928, Valid Loss: 0.8266\n",
      "Model Num 7 (DLinear), Epoch 10, Train Loss: 0.9928, Valid Loss: 0.8267\n",
      "Model Num 7 (DLinear), Epoch 11, Train Loss: 0.9940, Valid Loss: 0.8277\n",
      "Early stopping for model 7 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_7 with validation loss: 0.8155\n",
      "------------------------------------------------------------\n",
      "Model Num 8 (DLinear), Epoch 1, Train Loss: 1.0210, Valid Loss: 0.8178\n",
      "Model Num 8 (DLinear), Epoch 2, Train Loss: 1.0022, Valid Loss: 0.8242\n",
      "Model Num 8 (DLinear), Epoch 3, Train Loss: 0.9992, Valid Loss: 0.8288\n",
      "Model Num 8 (DLinear), Epoch 4, Train Loss: 1.0010, Valid Loss: 0.8282\n",
      "Model Num 8 (DLinear), Epoch 5, Train Loss: 0.9998, Valid Loss: 0.8290\n",
      "Model Num 8 (DLinear), Epoch 6, Train Loss: 0.9991, Valid Loss: 0.8317\n",
      "Model Num 8 (DLinear), Epoch 7, Train Loss: 0.9989, Valid Loss: 0.8322\n",
      "Model Num 8 (DLinear), Epoch 8, Train Loss: 0.9992, Valid Loss: 0.8323\n",
      "Model Num 8 (DLinear), Epoch 9, Train Loss: 0.9981, Valid Loss: 0.8302\n",
      "Model Num 8 (DLinear), Epoch 10, Train Loss: 0.9991, Valid Loss: 0.8306\n",
      "Model Num 8 (DLinear), Epoch 11, Train Loss: 0.9992, Valid Loss: 0.8298\n",
      "Early stopping for model 8 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_8 with validation loss: 0.8178\n",
      "------------------------------------------------------------\n",
      "Model Num 9 (DLinear), Epoch 1, Train Loss: 1.0193, Valid Loss: 0.8198\n",
      "Model Num 9 (DLinear), Epoch 2, Train Loss: 1.0033, Valid Loss: 0.8240\n",
      "Model Num 9 (DLinear), Epoch 3, Train Loss: 1.0025, Valid Loss: 0.8279\n",
      "Model Num 9 (DLinear), Epoch 4, Train Loss: 1.0047, Valid Loss: 0.8286\n",
      "Model Num 9 (DLinear), Epoch 5, Train Loss: 1.0024, Valid Loss: 0.8305\n",
      "Model Num 9 (DLinear), Epoch 6, Train Loss: 1.0014, Valid Loss: 0.8288\n",
      "Model Num 9 (DLinear), Epoch 7, Train Loss: 1.0008, Valid Loss: 0.8297\n",
      "Model Num 9 (DLinear), Epoch 8, Train Loss: 1.0017, Valid Loss: 0.8296\n",
      "Model Num 9 (DLinear), Epoch 9, Train Loss: 1.0019, Valid Loss: 0.8292\n",
      "Model Num 9 (DLinear), Epoch 10, Train Loss: 1.0024, Valid Loss: 0.8298\n",
      "Model Num 9 (DLinear), Epoch 11, Train Loss: 1.0019, Valid Loss: 0.8301\n",
      "Early stopping for model 9 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_9 with validation loss: 0.8198\n",
      "------------------------------------------------------------\n",
      "Model Num 10 (DLinear), Epoch 1, Train Loss: 1.0129, Valid Loss: 0.8164\n",
      "Model Num 10 (DLinear), Epoch 2, Train Loss: 0.9950, Valid Loss: 0.8233\n",
      "Model Num 10 (DLinear), Epoch 3, Train Loss: 0.9923, Valid Loss: 0.8265\n",
      "Model Num 10 (DLinear), Epoch 4, Train Loss: 0.9945, Valid Loss: 0.8283\n",
      "Model Num 10 (DLinear), Epoch 5, Train Loss: 0.9918, Valid Loss: 0.8277\n",
      "Model Num 10 (DLinear), Epoch 6, Train Loss: 0.9915, Valid Loss: 0.8283\n",
      "Model Num 10 (DLinear), Epoch 7, Train Loss: 0.9914, Valid Loss: 0.8280\n",
      "Model Num 10 (DLinear), Epoch 8, Train Loss: 0.9918, Valid Loss: 0.8302\n",
      "Model Num 10 (DLinear), Epoch 9, Train Loss: 0.9963, Valid Loss: 0.8290\n",
      "Model Num 10 (DLinear), Epoch 10, Train Loss: 0.9949, Valid Loss: 0.8293\n",
      "Model Num 10 (DLinear), Epoch 11, Train Loss: 0.9925, Valid Loss: 0.8305\n",
      "Early stopping for model 10 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_10 with validation loss: 0.8164\n",
      "------------------------------------------------------------\n",
      "Model Num 11 (DLinear), Epoch 1, Train Loss: 1.0060, Valid Loss: 0.8158\n",
      "Model Num 11 (DLinear), Epoch 2, Train Loss: 0.9864, Valid Loss: 0.8221\n",
      "Model Num 11 (DLinear), Epoch 3, Train Loss: 0.9840, Valid Loss: 0.8249\n",
      "Model Num 11 (DLinear), Epoch 4, Train Loss: 0.9838, Valid Loss: 0.8268\n",
      "Model Num 11 (DLinear), Epoch 5, Train Loss: 0.9842, Valid Loss: 0.8281\n",
      "Model Num 11 (DLinear), Epoch 6, Train Loss: 0.9850, Valid Loss: 0.8271\n",
      "Model Num 11 (DLinear), Epoch 7, Train Loss: 0.9842, Valid Loss: 0.8274\n",
      "Model Num 11 (DLinear), Epoch 8, Train Loss: 0.9848, Valid Loss: 0.8277\n",
      "Model Num 11 (DLinear), Epoch 9, Train Loss: 0.9852, Valid Loss: 0.8277\n",
      "Model Num 11 (DLinear), Epoch 10, Train Loss: 0.9850, Valid Loss: 0.8285\n",
      "Model Num 11 (DLinear), Epoch 11, Train Loss: 0.9847, Valid Loss: 0.8271\n",
      "Early stopping for model 11 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_11 with validation loss: 0.8158\n",
      "------------------------------------------------------------\n",
      "Model Num 12 (DLinear), Epoch 1, Train Loss: 1.0014, Valid Loss: 0.8158\n",
      "Model Num 12 (DLinear), Epoch 2, Train Loss: 0.9827, Valid Loss: 0.8216\n",
      "Model Num 12 (DLinear), Epoch 3, Train Loss: 0.9802, Valid Loss: 0.8238\n",
      "Model Num 12 (DLinear), Epoch 4, Train Loss: 0.9823, Valid Loss: 0.8272\n",
      "Model Num 12 (DLinear), Epoch 5, Train Loss: 0.9800, Valid Loss: 0.8280\n",
      "Model Num 12 (DLinear), Epoch 6, Train Loss: 0.9802, Valid Loss: 0.8294\n",
      "Model Num 12 (DLinear), Epoch 7, Train Loss: 0.9795, Valid Loss: 0.8293\n",
      "Model Num 12 (DLinear), Epoch 8, Train Loss: 0.9808, Valid Loss: 0.8302\n",
      "Model Num 12 (DLinear), Epoch 9, Train Loss: 0.9815, Valid Loss: 0.8300\n",
      "Model Num 12 (DLinear), Epoch 10, Train Loss: 0.9804, Valid Loss: 0.8293\n",
      "Model Num 12 (DLinear), Epoch 11, Train Loss: 0.9794, Valid Loss: 0.8282\n",
      "Early stopping for model 12 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_12 with validation loss: 0.8158\n",
      "------------------------------------------------------------\n",
      "Model Num 13 (DLinear), Epoch 1, Train Loss: 1.0329, Valid Loss: 0.8135\n",
      "Model Num 13 (DLinear), Epoch 2, Train Loss: 1.0181, Valid Loss: 0.8195\n",
      "Model Num 13 (DLinear), Epoch 3, Train Loss: 1.0145, Valid Loss: 0.8229\n",
      "Model Num 13 (DLinear), Epoch 4, Train Loss: 1.0148, Valid Loss: 0.8217\n",
      "Model Num 13 (DLinear), Epoch 5, Train Loss: 1.0138, Valid Loss: 0.8260\n",
      "Model Num 13 (DLinear), Epoch 6, Train Loss: 1.0176, Valid Loss: 0.8229\n",
      "Model Num 13 (DLinear), Epoch 7, Train Loss: 1.0163, Valid Loss: 0.8242\n",
      "Model Num 13 (DLinear), Epoch 8, Train Loss: 1.0128, Valid Loss: 0.8247\n",
      "Model Num 13 (DLinear), Epoch 9, Train Loss: 1.0193, Valid Loss: 0.8250\n",
      "Model Num 13 (DLinear), Epoch 10, Train Loss: 1.0181, Valid Loss: 0.8251\n",
      "Model Num 13 (DLinear), Epoch 11, Train Loss: 1.0168, Valid Loss: 0.8243\n",
      "Early stopping for model 13 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_13 with validation loss: 0.8135\n",
      "------------------------------------------------------------\n",
      "Model Num 14 (DLinear), Epoch 1, Train Loss: 1.0290, Valid Loss: 0.8151\n",
      "Model Num 14 (DLinear), Epoch 2, Train Loss: 1.0057, Valid Loss: 0.8215\n",
      "Model Num 14 (DLinear), Epoch 3, Train Loss: 1.0043, Valid Loss: 0.8255\n",
      "Model Num 14 (DLinear), Epoch 4, Train Loss: 1.0037, Valid Loss: 0.8254\n",
      "Model Num 14 (DLinear), Epoch 5, Train Loss: 1.0026, Valid Loss: 0.8271\n",
      "Model Num 14 (DLinear), Epoch 6, Train Loss: 1.0037, Valid Loss: 0.8282\n",
      "Model Num 14 (DLinear), Epoch 7, Train Loss: 1.0041, Valid Loss: 0.8276\n",
      "Model Num 14 (DLinear), Epoch 8, Train Loss: 1.0031, Valid Loss: 0.8278\n",
      "Model Num 14 (DLinear), Epoch 9, Train Loss: 1.0045, Valid Loss: 0.8273\n",
      "Model Num 14 (DLinear), Epoch 10, Train Loss: 1.0089, Valid Loss: 0.8280\n",
      "Model Num 14 (DLinear), Epoch 11, Train Loss: 1.0050, Valid Loss: 0.8279\n",
      "Early stopping for model 14 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_14 with validation loss: 0.8151\n",
      "------------------------------------------------------------\n",
      "Model Num 15 (DLinear), Epoch 1, Train Loss: 1.0692, Valid Loss: 0.8155\n",
      "Model Num 15 (DLinear), Epoch 2, Train Loss: 1.0475, Valid Loss: 0.8238\n",
      "Model Num 15 (DLinear), Epoch 3, Train Loss: 1.0448, Valid Loss: 0.8275\n",
      "Model Num 15 (DLinear), Epoch 4, Train Loss: 1.0445, Valid Loss: 0.8289\n",
      "Model Num 15 (DLinear), Epoch 5, Train Loss: 1.0451, Valid Loss: 0.8298\n",
      "Model Num 15 (DLinear), Epoch 6, Train Loss: 1.0456, Valid Loss: 0.8302\n",
      "Model Num 15 (DLinear), Epoch 7, Train Loss: 1.0470, Valid Loss: 0.8297\n",
      "Model Num 15 (DLinear), Epoch 8, Train Loss: 1.0573, Valid Loss: 0.8312\n",
      "Model Num 15 (DLinear), Epoch 9, Train Loss: 1.0481, Valid Loss: 0.8307\n",
      "Model Num 15 (DLinear), Epoch 10, Train Loss: 1.0449, Valid Loss: 0.8303\n",
      "Model Num 15 (DLinear), Epoch 11, Train Loss: 1.0472, Valid Loss: 0.8315\n",
      "Early stopping for model 15 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_15 with validation loss: 0.8155\n",
      "------------------------------------------------------------\n",
      "Model Num 16 (DLinear), Epoch 1, Train Loss: 0.9875, Valid Loss: 0.8176\n",
      "Model Num 16 (DLinear), Epoch 2, Train Loss: 0.9701, Valid Loss: 0.8218\n",
      "Model Num 16 (DLinear), Epoch 3, Train Loss: 0.9686, Valid Loss: 0.8242\n",
      "Model Num 16 (DLinear), Epoch 4, Train Loss: 0.9673, Valid Loss: 0.8257\n",
      "Model Num 16 (DLinear), Epoch 5, Train Loss: 0.9677, Valid Loss: 0.8273\n",
      "Model Num 16 (DLinear), Epoch 6, Train Loss: 0.9682, Valid Loss: 0.8262\n",
      "Model Num 16 (DLinear), Epoch 7, Train Loss: 0.9707, Valid Loss: 0.8269\n",
      "Model Num 16 (DLinear), Epoch 8, Train Loss: 0.9685, Valid Loss: 0.8261\n",
      "Model Num 16 (DLinear), Epoch 9, Train Loss: 0.9671, Valid Loss: 0.8272\n",
      "Model Num 16 (DLinear), Epoch 10, Train Loss: 0.9699, Valid Loss: 0.8261\n",
      "Model Num 16 (DLinear), Epoch 11, Train Loss: 0.9701, Valid Loss: 0.8262\n",
      "Early stopping for model 16 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_16 with validation loss: 0.8176\n",
      "------------------------------------------------------------\n",
      "Model Num 17 (DLinear), Epoch 1, Train Loss: 0.9521, Valid Loss: 0.8154\n",
      "Model Num 17 (DLinear), Epoch 2, Train Loss: 0.9364, Valid Loss: 0.8203\n",
      "Model Num 17 (DLinear), Epoch 3, Train Loss: 0.9341, Valid Loss: 0.8210\n",
      "Model Num 17 (DLinear), Epoch 4, Train Loss: 0.9346, Valid Loss: 0.8241\n",
      "Model Num 17 (DLinear), Epoch 5, Train Loss: 0.9353, Valid Loss: 0.8244\n",
      "Model Num 17 (DLinear), Epoch 6, Train Loss: 0.9353, Valid Loss: 0.8238\n",
      "Model Num 17 (DLinear), Epoch 7, Train Loss: 0.9363, Valid Loss: 0.8243\n",
      "Model Num 17 (DLinear), Epoch 8, Train Loss: 0.9376, Valid Loss: 0.8227\n",
      "Model Num 17 (DLinear), Epoch 9, Train Loss: 0.9350, Valid Loss: 0.8256\n",
      "Model Num 17 (DLinear), Epoch 10, Train Loss: 0.9349, Valid Loss: 0.8249\n",
      "Model Num 17 (DLinear), Epoch 11, Train Loss: 0.9351, Valid Loss: 0.8241\n",
      "Early stopping for model 17 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_17 with validation loss: 0.8154\n",
      "------------------------------------------------------------\n",
      "Model Num 18 (DLinear), Epoch 1, Train Loss: 1.0399, Valid Loss: 0.8170\n",
      "Model Num 18 (DLinear), Epoch 2, Train Loss: 1.0215, Valid Loss: 0.8218\n",
      "Model Num 18 (DLinear), Epoch 3, Train Loss: 1.0187, Valid Loss: 0.8254\n",
      "Model Num 18 (DLinear), Epoch 4, Train Loss: 1.0203, Valid Loss: 0.8262\n",
      "Model Num 18 (DLinear), Epoch 5, Train Loss: 1.0195, Valid Loss: 0.8273\n",
      "Model Num 18 (DLinear), Epoch 6, Train Loss: 1.0215, Valid Loss: 0.8286\n",
      "Model Num 18 (DLinear), Epoch 7, Train Loss: 1.0233, Valid Loss: 0.8281\n",
      "Model Num 18 (DLinear), Epoch 8, Train Loss: 1.0199, Valid Loss: 0.8295\n",
      "Model Num 18 (DLinear), Epoch 9, Train Loss: 1.0192, Valid Loss: 0.8276\n",
      "Model Num 18 (DLinear), Epoch 10, Train Loss: 1.0250, Valid Loss: 0.8286\n",
      "Model Num 18 (DLinear), Epoch 11, Train Loss: 1.0258, Valid Loss: 0.8281\n",
      "Early stopping for model 18 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_18 with validation loss: 0.8170\n",
      "------------------------------------------------------------\n",
      "Model Num 19 (DLinear), Epoch 1, Train Loss: 1.0177, Valid Loss: 0.8159\n",
      "Model Num 19 (DLinear), Epoch 2, Train Loss: 1.0007, Valid Loss: 0.8241\n",
      "Model Num 19 (DLinear), Epoch 3, Train Loss: 0.9997, Valid Loss: 0.8278\n",
      "Model Num 19 (DLinear), Epoch 4, Train Loss: 0.9996, Valid Loss: 0.8282\n",
      "Model Num 19 (DLinear), Epoch 5, Train Loss: 1.0011, Valid Loss: 0.8289\n",
      "Model Num 19 (DLinear), Epoch 6, Train Loss: 1.0003, Valid Loss: 0.8290\n",
      "Model Num 19 (DLinear), Epoch 7, Train Loss: 1.0015, Valid Loss: 0.8296\n",
      "Model Num 19 (DLinear), Epoch 8, Train Loss: 0.9999, Valid Loss: 0.8304\n",
      "Model Num 19 (DLinear), Epoch 9, Train Loss: 0.9986, Valid Loss: 0.8303\n",
      "Model Num 19 (DLinear), Epoch 10, Train Loss: 0.9998, Valid Loss: 0.8298\n",
      "Model Num 19 (DLinear), Epoch 11, Train Loss: 0.9989, Valid Loss: 0.8284\n",
      "Early stopping for model 19 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_19 with validation loss: 0.8159\n",
      "------------------------------------------------------------\n",
      "Model Num 20 (DLinear), Epoch 1, Train Loss: 0.9751, Valid Loss: 0.8161\n",
      "Model Num 20 (DLinear), Epoch 2, Train Loss: 0.9607, Valid Loss: 0.8218\n",
      "Model Num 20 (DLinear), Epoch 3, Train Loss: 0.9600, Valid Loss: 0.8252\n",
      "Model Num 20 (DLinear), Epoch 4, Train Loss: 0.9593, Valid Loss: 0.8282\n",
      "Model Num 20 (DLinear), Epoch 5, Train Loss: 0.9604, Valid Loss: 0.8262\n",
      "Model Num 20 (DLinear), Epoch 6, Train Loss: 0.9625, Valid Loss: 0.8264\n",
      "Model Num 20 (DLinear), Epoch 7, Train Loss: 0.9602, Valid Loss: 0.8276\n",
      "Model Num 20 (DLinear), Epoch 8, Train Loss: 0.9657, Valid Loss: 0.8262\n",
      "Model Num 20 (DLinear), Epoch 9, Train Loss: 0.9629, Valid Loss: 0.8271\n",
      "Model Num 20 (DLinear), Epoch 10, Train Loss: 0.9592, Valid Loss: 0.8272\n",
      "Model Num 20 (DLinear), Epoch 11, Train Loss: 0.9609, Valid Loss: 0.8274\n",
      "Early stopping for model 20 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_20 with validation loss: 0.8161\n",
      "------------------------------------------------------------\n",
      "Model Num 21 (DLinear), Epoch 1, Train Loss: 1.0562, Valid Loss: 0.8157\n",
      "Model Num 21 (DLinear), Epoch 2, Train Loss: 1.0353, Valid Loss: 0.8235\n",
      "Model Num 21 (DLinear), Epoch 3, Train Loss: 1.0311, Valid Loss: 0.8271\n",
      "Model Num 21 (DLinear), Epoch 4, Train Loss: 1.0308, Valid Loss: 0.8303\n",
      "Model Num 21 (DLinear), Epoch 5, Train Loss: 1.0314, Valid Loss: 0.8299\n",
      "Model Num 21 (DLinear), Epoch 6, Train Loss: 1.0314, Valid Loss: 0.8294\n",
      "Model Num 21 (DLinear), Epoch 7, Train Loss: 1.0312, Valid Loss: 0.8302\n",
      "Model Num 21 (DLinear), Epoch 8, Train Loss: 1.0319, Valid Loss: 0.8302\n",
      "Model Num 21 (DLinear), Epoch 9, Train Loss: 1.0336, Valid Loss: 0.8296\n",
      "Model Num 21 (DLinear), Epoch 10, Train Loss: 1.0319, Valid Loss: 0.8310\n",
      "Model Num 21 (DLinear), Epoch 11, Train Loss: 1.0331, Valid Loss: 0.8288\n",
      "Early stopping for model 21 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_21 with validation loss: 0.8157\n",
      "------------------------------------------------------------\n",
      "Model Num 22 (DLinear), Epoch 1, Train Loss: 1.0315, Valid Loss: 0.8157\n",
      "Model Num 22 (DLinear), Epoch 2, Train Loss: 1.0115, Valid Loss: 0.8224\n",
      "Model Num 22 (DLinear), Epoch 3, Train Loss: 1.0168, Valid Loss: 0.8260\n",
      "Model Num 22 (DLinear), Epoch 4, Train Loss: 1.0105, Valid Loss: 0.8258\n",
      "Model Num 22 (DLinear), Epoch 5, Train Loss: 1.0137, Valid Loss: 0.8267\n",
      "Model Num 22 (DLinear), Epoch 6, Train Loss: 1.0102, Valid Loss: 0.8272\n",
      "Model Num 22 (DLinear), Epoch 7, Train Loss: 1.0106, Valid Loss: 0.8277\n",
      "Model Num 22 (DLinear), Epoch 8, Train Loss: 1.0107, Valid Loss: 0.8271\n",
      "Model Num 22 (DLinear), Epoch 9, Train Loss: 1.0095, Valid Loss: 0.8278\n",
      "Model Num 22 (DLinear), Epoch 10, Train Loss: 1.0170, Valid Loss: 0.8279\n",
      "Model Num 22 (DLinear), Epoch 11, Train Loss: 1.0135, Valid Loss: 0.8282\n",
      "Early stopping for model 22 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_22 with validation loss: 0.8157\n",
      "------------------------------------------------------------\n",
      "Model Num 23 (DLinear), Epoch 1, Train Loss: 0.9835, Valid Loss: 0.8172\n",
      "Model Num 23 (DLinear), Epoch 2, Train Loss: 0.9684, Valid Loss: 0.8239\n",
      "Model Num 23 (DLinear), Epoch 3, Train Loss: 0.9660, Valid Loss: 0.8271\n",
      "Model Num 23 (DLinear), Epoch 4, Train Loss: 0.9657, Valid Loss: 0.8274\n",
      "Model Num 23 (DLinear), Epoch 5, Train Loss: 0.9675, Valid Loss: 0.8282\n",
      "Model Num 23 (DLinear), Epoch 6, Train Loss: 0.9661, Valid Loss: 0.8296\n",
      "Model Num 23 (DLinear), Epoch 7, Train Loss: 0.9673, Valid Loss: 0.8303\n",
      "Model Num 23 (DLinear), Epoch 8, Train Loss: 0.9670, Valid Loss: 0.8282\n",
      "Model Num 23 (DLinear), Epoch 9, Train Loss: 0.9672, Valid Loss: 0.8280\n",
      "Model Num 23 (DLinear), Epoch 10, Train Loss: 0.9653, Valid Loss: 0.8288\n",
      "Model Num 23 (DLinear), Epoch 11, Train Loss: 0.9660, Valid Loss: 0.8285\n",
      "Early stopping for model 23 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_23 with validation loss: 0.8172\n",
      "------------------------------------------------------------\n",
      "Model Num 24 (DLinear), Epoch 1, Train Loss: 1.0133, Valid Loss: 0.8167\n",
      "Model Num 24 (DLinear), Epoch 2, Train Loss: 0.9910, Valid Loss: 0.8222\n",
      "Model Num 24 (DLinear), Epoch 3, Train Loss: 0.9873, Valid Loss: 0.8256\n",
      "Model Num 24 (DLinear), Epoch 4, Train Loss: 0.9882, Valid Loss: 0.8281\n",
      "Model Num 24 (DLinear), Epoch 5, Train Loss: 0.9883, Valid Loss: 0.8278\n",
      "Model Num 24 (DLinear), Epoch 6, Train Loss: 0.9884, Valid Loss: 0.8270\n",
      "Model Num 24 (DLinear), Epoch 7, Train Loss: 0.9908, Valid Loss: 0.8277\n",
      "Model Num 24 (DLinear), Epoch 8, Train Loss: 0.9888, Valid Loss: 0.8274\n",
      "Model Num 24 (DLinear), Epoch 9, Train Loss: 0.9879, Valid Loss: 0.8280\n",
      "Model Num 24 (DLinear), Epoch 10, Train Loss: 0.9885, Valid Loss: 0.8276\n",
      "Model Num 24 (DLinear), Epoch 11, Train Loss: 0.9884, Valid Loss: 0.8285\n",
      "Early stopping for model 24 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_24 with validation loss: 0.8167\n",
      "------------------------------------------------------------\n",
      "Model Num 25 (DLinear), Epoch 1, Train Loss: 1.0073, Valid Loss: 0.8153\n",
      "Model Num 25 (DLinear), Epoch 2, Train Loss: 0.9874, Valid Loss: 0.8217\n",
      "Model Num 25 (DLinear), Epoch 3, Train Loss: 0.9848, Valid Loss: 0.8250\n",
      "Model Num 25 (DLinear), Epoch 4, Train Loss: 0.9841, Valid Loss: 0.8273\n",
      "Model Num 25 (DLinear), Epoch 5, Train Loss: 0.9855, Valid Loss: 0.8281\n",
      "Model Num 25 (DLinear), Epoch 6, Train Loss: 0.9836, Valid Loss: 0.8281\n",
      "Model Num 25 (DLinear), Epoch 7, Train Loss: 0.9824, Valid Loss: 0.8295\n",
      "Model Num 25 (DLinear), Epoch 8, Train Loss: 0.9835, Valid Loss: 0.8300\n",
      "Model Num 25 (DLinear), Epoch 9, Train Loss: 0.9839, Valid Loss: 0.8289\n",
      "Model Num 25 (DLinear), Epoch 10, Train Loss: 0.9832, Valid Loss: 0.8308\n",
      "Model Num 25 (DLinear), Epoch 11, Train Loss: 0.9841, Valid Loss: 0.8286\n",
      "Early stopping for model 25 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_25 with validation loss: 0.8153\n",
      "------------------------------------------------------------\n",
      "Model Num 26 (DLinear), Epoch 1, Train Loss: 0.9669, Valid Loss: 0.8167\n",
      "Model Num 26 (DLinear), Epoch 2, Train Loss: 0.9458, Valid Loss: 0.8232\n",
      "Model Num 26 (DLinear), Epoch 3, Train Loss: 0.9435, Valid Loss: 0.8245\n",
      "Model Num 26 (DLinear), Epoch 4, Train Loss: 0.9437, Valid Loss: 0.8263\n",
      "Model Num 26 (DLinear), Epoch 5, Train Loss: 0.9492, Valid Loss: 0.8263\n",
      "Model Num 26 (DLinear), Epoch 6, Train Loss: 0.9478, Valid Loss: 0.8299\n",
      "Model Num 26 (DLinear), Epoch 7, Train Loss: 0.9442, Valid Loss: 0.8289\n",
      "Model Num 26 (DLinear), Epoch 8, Train Loss: 0.9453, Valid Loss: 0.8267\n",
      "Model Num 26 (DLinear), Epoch 9, Train Loss: 0.9438, Valid Loss: 0.8277\n",
      "Model Num 26 (DLinear), Epoch 10, Train Loss: 0.9464, Valid Loss: 0.8275\n",
      "Model Num 26 (DLinear), Epoch 11, Train Loss: 0.9439, Valid Loss: 0.8272\n",
      "Early stopping for model 26 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_26 with validation loss: 0.8167\n",
      "------------------------------------------------------------\n",
      "Model Num 27 (DLinear), Epoch 1, Train Loss: 0.9845, Valid Loss: 0.8151\n",
      "Model Num 27 (DLinear), Epoch 2, Train Loss: 0.9683, Valid Loss: 0.8211\n",
      "Model Num 27 (DLinear), Epoch 3, Train Loss: 0.9656, Valid Loss: 0.8247\n",
      "Model Num 27 (DLinear), Epoch 4, Train Loss: 0.9671, Valid Loss: 0.8244\n",
      "Model Num 27 (DLinear), Epoch 5, Train Loss: 0.9652, Valid Loss: 0.8262\n",
      "Model Num 27 (DLinear), Epoch 6, Train Loss: 0.9665, Valid Loss: 0.8263\n",
      "Model Num 27 (DLinear), Epoch 7, Train Loss: 0.9665, Valid Loss: 0.8261\n",
      "Model Num 27 (DLinear), Epoch 8, Train Loss: 0.9663, Valid Loss: 0.8254\n",
      "Model Num 27 (DLinear), Epoch 9, Train Loss: 0.9670, Valid Loss: 0.8261\n",
      "Model Num 27 (DLinear), Epoch 10, Train Loss: 0.9682, Valid Loss: 0.8246\n",
      "Model Num 27 (DLinear), Epoch 11, Train Loss: 0.9658, Valid Loss: 0.8257\n",
      "Early stopping for model 27 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_27 with validation loss: 0.8151\n",
      "------------------------------------------------------------\n",
      "Model Num 28 (DLinear), Epoch 1, Train Loss: 0.9813, Valid Loss: 0.8129\n",
      "Model Num 28 (DLinear), Epoch 2, Train Loss: 0.9678, Valid Loss: 0.8193\n",
      "Model Num 28 (DLinear), Epoch 3, Train Loss: 0.9659, Valid Loss: 0.8222\n",
      "Model Num 28 (DLinear), Epoch 4, Train Loss: 0.9677, Valid Loss: 0.8207\n",
      "Model Num 28 (DLinear), Epoch 5, Train Loss: 0.9658, Valid Loss: 0.8212\n",
      "Model Num 28 (DLinear), Epoch 6, Train Loss: 0.9663, Valid Loss: 0.8208\n",
      "Model Num 28 (DLinear), Epoch 7, Train Loss: 0.9683, Valid Loss: 0.8205\n",
      "Model Num 28 (DLinear), Epoch 8, Train Loss: 0.9668, Valid Loss: 0.8215\n",
      "Model Num 28 (DLinear), Epoch 9, Train Loss: 0.9658, Valid Loss: 0.8212\n",
      "Model Num 28 (DLinear), Epoch 10, Train Loss: 0.9661, Valid Loss: 0.8211\n",
      "Model Num 28 (DLinear), Epoch 11, Train Loss: 0.9652, Valid Loss: 0.8214\n",
      "Early stopping for model 28 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_28 with validation loss: 0.8129\n",
      "------------------------------------------------------------\n",
      "Model Num 29 (DLinear), Epoch 1, Train Loss: 1.0000, Valid Loss: 0.8144\n",
      "Model Num 29 (DLinear), Epoch 2, Train Loss: 0.9806, Valid Loss: 0.8226\n",
      "Model Num 29 (DLinear), Epoch 3, Train Loss: 0.9807, Valid Loss: 0.8255\n",
      "Model Num 29 (DLinear), Epoch 4, Train Loss: 0.9790, Valid Loss: 0.8265\n",
      "Model Num 29 (DLinear), Epoch 5, Train Loss: 0.9806, Valid Loss: 0.8275\n",
      "Model Num 29 (DLinear), Epoch 6, Train Loss: 0.9807, Valid Loss: 0.8276\n",
      "Model Num 29 (DLinear), Epoch 7, Train Loss: 0.9791, Valid Loss: 0.8284\n",
      "Model Num 29 (DLinear), Epoch 8, Train Loss: 0.9794, Valid Loss: 0.8272\n",
      "Model Num 29 (DLinear), Epoch 9, Train Loss: 0.9768, Valid Loss: 0.8283\n",
      "Model Num 29 (DLinear), Epoch 10, Train Loss: 0.9779, Valid Loss: 0.8274\n",
      "Model Num 29 (DLinear), Epoch 11, Train Loss: 0.9777, Valid Loss: 0.8279\n",
      "Early stopping for model 29 (DLinear) at epoch 11!\n",
      "Loading best model for DLinear_29 with validation loss: 0.8144\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dlinear_models, dlinear_indices_bootstrap = train(\n",
    "    model_cls=DLinear,\n",
    "    data_loader=bootstrap_loaders,\n",
    "    valid_data_loader=valid_loader,\n",
    "    EPOCHS = 20,\n",
    "    valid_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee61076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def compute_train_residuals_variable_wise(model_type, train_dataset, bootstrap_models, bootstrap_index_sets, device='cuda'):\n",
    "#     \"\"\"\n",
    "#     훈련 데이터셋에 대한 out-of-bag 잔차(residuals)를 계산합니다.\n",
    "\n",
    "#     Args:\n",
    "#         model_type (str): 모델의 종류 ('MLP' 또는 'DLinear').\n",
    "#         train_dataset (Dataset): 훈련 데이터셋. inverse_transform 메소드를 포함해야 합니다.\n",
    "#         bootstrap_models (list): 부트스트랩으로 훈련된 모델들의 리스트.\n",
    "#         bootstrap_index_sets (list): 각 부트스트랩 모델이 학습에 사용한 데이터 인덱스 set의 리스트.\n",
    "#         device (str): 연산에 사용할 디바이스 ('cuda' 또는 'cpu').\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (전체 예측값 텐서, 전체 실제값 텐서, 잔차 텐서)\n",
    "#                (torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "#     \"\"\"\n",
    "#     T = len(train_dataset)\n",
    "#     if T == 0:\n",
    "#         # 데이터셋이 비어있는 경우 처리\n",
    "#         return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "\n",
    "#     # shape 정보를 명확하게 얻기 위해 샘플 하나를 사용\n",
    "#     sample_x, sample_y = train_dataset[0][:2]\n",
    "#     pred_len, num_features = torch.tensor(sample_y).shape\n",
    "\n",
    "#     # 1. 오타 수정: train_set -> train_dataset\n",
    "#     # 2. shape 정보를 변수로 만들어 가독성 향상\n",
    "#     residuals = torch.zeros(T, pred_len, num_features).to(device)\n",
    "#     sample_indices = np.random.choice(T+1, num_samples, replace=False)\n",
    "#     whole_pred_inv = []\n",
    "#     whole_true_inv = []\n",
    "    \n",
    "#     for i in tqdm(range(T), desc='Computing Residuals'):\n",
    "        \n",
    "#         if i in sample_indices:\n",
    "#             seq_x, seq_y, _, _ = train_dataset[i]\n",
    "            \n",
    "#             y_true = torch.tensor(seq_y).float().to(device)  # [pred_len, num_features]\n",
    "\n",
    "#             # --- 3. 중복 코드 개선: 모델 타입에 따라 입력 형태만 다르게 처리 ---\n",
    "#             if model_type == 'MLP':\n",
    "#                 x_input = torch.tensor(seq_x).float().flatten().unsqueeze(0).to(device)  # [1, D]\n",
    "#             elif model_type == 'DLinear':\n",
    "#                 x_input = torch.tensor(seq_x).float().unsqueeze(0).to(device)  # [1, seq_len, num_features]\n",
    "#             else:\n",
    "#                 raise ValueError(f\"지원하지 않는 모델 타입입니다: {model_type}\")\n",
    "#             # -----------------------------------------------------------------\n",
    "\n",
    "#             preds = []\n",
    "#             for b, model in enumerate(bootstrap_models):\n",
    "#                 if i not in bootstrap_index_sets[b]:\n",
    "#                     model.eval()\n",
    "#                     with torch.no_grad():\n",
    "#                         y_pred = model(x_input).squeeze(0)\n",
    "#                         if model_type == 'MLP' and len(y_pred.shape) == 1:\n",
    "#                             y_pred = y_pred.view_as(y_true)\n",
    "#                         preds.append(y_pred)\n",
    "\n",
    "#             if len(preds) == 0:\n",
    "\n",
    "#                 residuals[i] = torch.full((pred_len, num_features), float('nan')).to(device)\n",
    "#             else:\n",
    "#                 pred_mean = torch.stack(preds).mean(dim=0)  # [pred_len, num_features]\n",
    "\n",
    "#                 y_true_np = y_true.cpu().numpy()\n",
    "#                 pred_np = pred_mean.cpu().numpy()\n",
    "\n",
    "#                 y_true_inv = torch.tensor(train_dataset.inverse_transform(y_true_np)).to(device)\n",
    "#                 pred_inv = torch.tensor(train_dataset.inverse_transform(pred_np)).to(device)\n",
    "\n",
    "#                 # 절대 오차(잔차) 계산\n",
    "#                 residuals[i] = torch.abs(y_true_inv - pred_inv)  # [pred_len, num_features]\n",
    "                \n",
    "#                 whole_pred_inv.append(pred_inv)\n",
    "#                 whole_true_inv.append(y_true_inv)\n",
    "\n",
    "#             stacked_preds = torch.stack(whole_pred_inv)\n",
    "#             stacked_trues = torch.stack(whole_true_inv)\n",
    "            \n",
    "#             # squared_error = (stacked_preds - stacked_trues)**2\n",
    "#             # mse = squared_error.mean() \n",
    "#             # print(f'{model_type} Test Loss: {mse:.4f}')\n",
    "            \n",
    "#         if not whole_pred_inv:\n",
    "#             empty_tensor = torch.empty(0, pred_len, num_features).to(device)\n",
    "#             return empty_tensor, empty_tensor, residuals\n",
    "        \n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "#     # return torch.stack(whole_pred_inv), torch.stack(whole_true_inv), residuals\n",
    "#     return stacked_preds, stacked_trues, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c07e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506ce6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_train_residuals_variable_wise(model_type, train_dataset, bootstrap_models, bootstrap_index_sets, num_samples=252, device='cuda'):\n",
    "    \"\"\"\n",
    "    훈련 데이터셋에 대한 out-of-bag 잔차(residuals)를 올바르게 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): 모델의 종류 ('MLP' 또는 'DLinear').\n",
    "        train_dataset (Dataset): 훈련 데이터셋. inverse_transform 메소드를 포함해야 합니다.\n",
    "        bootstrap_models (list): 부트스트랩으로 훈련된 모델들의 리스트.\n",
    "        bootstrap_index_sets (list): 각 부트스트랩 모델이 학습에 사용한 데이터 인덱스 set의 리스트.\n",
    "        num_samples (int): 잔차를 계산할 훈련 데이터 샘플의 수.\n",
    "        device (str): 연산에 사용할 디바이스 ('cuda' 또는 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (전체 예측값 텐서, 전체 실제값 텐서, 잔차 텐서)\n",
    "               (torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "    T = len(train_dataset)\n",
    "    if T == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "\n",
    "    # shape 정보를 명확하게 얻기 위해 샘플 하나를 사용\n",
    "    sample_x, sample_y = train_dataset[0][:2]\n",
    "    # torch.tensor()로 감싸서 shape을 안전하게 추출\n",
    "    pred_len, num_features = torch.tensor(sample_y).shape\n",
    "\n",
    "    # [수정 1] 샘플링을 루프 밖에서 한 번만 수행합니다.\n",
    "    # T보다 num_samples가 클 경우 T로 제한하여 모든 샘플을 사용하도록 합니다.\n",
    "    actual_num_samples = min(num_samples, T)\n",
    "    sample_indices = np.random.choice(T, actual_num_samples, replace=False)\n",
    "\n",
    "    # 처리된 샘플만 저장할 리스트\n",
    "    whole_pred_inv = []\n",
    "    whole_true_inv = []\n",
    "    \n",
    "    # [수정 2] 잔차 텐서의 크기를 실제 샘플 수에 맞게 조정하고, 모든 값을 NaN으로 초기화\n",
    "    # 이렇게 하면 OOB 모델이 없는 경우를 자연스럽게 처리할 수 있습니다.\n",
    "    residuals_list = []\n",
    "\n",
    "    # [수정 3] tqdm의 total을 실제 샘플 수로 설정하여 진행률을 정확하게 표시합니다.\n",
    "    for i in tqdm(sample_indices, desc='Computing OOB Residuals'):\n",
    "        seq_x, seq_y, _, _ = train_dataset[i]\n",
    "        \n",
    "        y_true = torch.tensor(seq_y, dtype=torch.float32).to(device)  # [pred_len, num_features]\n",
    "\n",
    "        # 모델 타입에 따라 입력 형태 처리\n",
    "        if model_type == 'MLP':\n",
    "            x_input = torch.tensor(seq_x, dtype=torch.float32).flatten().unsqueeze(0).to(device)\n",
    "        elif model_type == 'DLinear':\n",
    "            x_input = torch.tensor(seq_x, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 모델 타입입니다: {model_type}\")\n",
    "\n",
    "        oob_preds = []\n",
    "        for b, model in enumerate(bootstrap_models):\n",
    "            # 이 샘플(i)을 훈련에 사용하지 않은 모델(OOB model)을 찾습니다.\n",
    "            if i not in bootstrap_index_sets[b]:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred = model(x_input).squeeze(0)\n",
    "                    # MLP 출력이 1D 벡터일 경우 올바른 shape으로 변환\n",
    "                    if model_type == 'MLP' and len(y_pred.shape) == 1:\n",
    "                        y_pred = y_pred.view_as(y_true)\n",
    "                    oob_preds.append(y_pred)\n",
    "\n",
    "        # OOB 예측을 수행한 모델이 하나라도 있는 경우에만 잔차를 계산합니다.\n",
    "        if len(oob_preds) > 0:\n",
    "            pred_mean = torch.stack(oob_preds).mean(dim=0)  # [pred_len, num_features]\n",
    "\n",
    "            # inverse_transform을 위해 numpy로 변환\n",
    "            y_true_np = y_true.cpu().numpy()\n",
    "            pred_np = pred_mean.cpu().numpy()\n",
    "\n",
    "            # 스케일링 복원\n",
    "            y_true_inv = torch.tensor(train_dataset.inverse_transform(y_true_np), dtype=torch.float32).to(device)\n",
    "            pred_inv = torch.tensor(train_dataset.inverse_transform(pred_np), dtype=torch.float32).to(device)\n",
    "\n",
    "            # 절대 오차(잔차) 계산 및 리스트에 추가\n",
    "            residual_val = torch.abs(y_true_inv - pred_inv)\n",
    "            residuals_list.append(residual_val)\n",
    "            \n",
    "            # 전체 예측값과 실제값 저장\n",
    "            whole_pred_inv.append(pred_inv)\n",
    "            whole_true_inv.append(y_true_inv)\n",
    "\n",
    "    # [수정 4] 루프가 모두 끝난 후, 수집된 리스트를 텐서로 변환합니다.\n",
    "    # 리스트가 비어있을 경우 (처리된 샘플이 하나도 없을 경우) 빈 텐서를 반환합니다.\n",
    "    if not whole_pred_inv:\n",
    "        empty_tensor = torch.empty(0, pred_len, num_features).to(device)\n",
    "        return empty_tensor, empty_tensor, empty_tensor\n",
    "        \n",
    "    stacked_preds = torch.stack(whole_pred_inv)\n",
    "    stacked_trues = torch.stack(whole_true_inv)\n",
    "    stacked_residuals = torch.stack(residuals_list)\n",
    "    \n",
    "    # MSE 계산 (필요시 주석 해제)\n",
    "    # mse = (stacked_preds - stacked_trues).pow(2).mean()\n",
    "    # print(f'{model_type} OOB MSE: {mse:.4f}')\n",
    "\n",
    "    return stacked_preds, stacked_trues, stacked_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffc1a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing OOB Residuals: 100%|██████████| 252/252 [00:01<00:00, 193.72it/s]\n",
      "Computing OOB Residuals: 100%|██████████| 252/252 [00:01<00:00, 178.45it/s]\n"
     ]
    }
   ],
   "source": [
    "var_train_preds, var_train_trues, var_train_residuals = compute_train_residuals_variable_wise('MLP', train_set, models, indices_bootstrap)\n",
    "dlinear_train_preds, dlinear_train_trues, dlinear_train_residuals = compute_train_residuals_variable_wise('DLinear', train_set, dlinear_models, dlinear_indices_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee94d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004, device='cuda:0')\n",
      "tensor(0.0004, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def mse(trues, preds):\n",
    "    squared_error = (trues-preds)**2\n",
    "    \n",
    "    return squared_error.mean()\n",
    "\n",
    "mlp_train_mse = mse(var_train_preds , var_train_trues)\n",
    "dlinear_train_mse = mse(dlinear_train_preds, dlinear_train_trues)\n",
    "\n",
    "print(mlp_train_mse)\n",
    "print(dlinear_train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660c33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_interval(\n",
    "    model_type,\n",
    "    test_dataset,\n",
    "    train_dataset,  # inverse_transform을 위해 필요\n",
    "    train_residuals,\n",
    "    bootstrap_models,\n",
    "    alpha=0.1,\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    테스트 데이터셋에 대한 예측 구간을 온라인 업데이트 방식으로 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        model_type (str): 모델의 종류 ('MLP' 또는 'DLinear').\n",
    "        test_dataset (Dataset): 테스트 데이터셋.\n",
    "        train_dataset (Dataset): 스케일러의 inverse_transform을 위해 필요한 훈련 데이터셋.\n",
    "        train_residuals (torch.Tensor): 사전에 계산된 훈련 데이터의 잔차 풀.\n",
    "        bootstrap_models (list): 앙상블을 위한 부트스트랩 모델 리스트.\n",
    "        alpha (float): 유의 수준 (e.g., 0.1 for 90% prediction interval).\n",
    "        device (str): 연산에 사용할 디바이스.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (상단 구간, 하단 구간, 예측값, 실제값) 텐서들.\n",
    "    \"\"\"\n",
    "    upper_intervals = []\n",
    "    lower_intervals = []\n",
    "    test_preds = []\n",
    "    test_trues = []\n",
    "    T = len(test_dataset)\n",
    "\n",
    "    # train_preds 인수는 사용되지 않아 제거했습니다.\n",
    "    # f_t 변수 또한 사용되지 않아 관련 코드를 제거했습니다.\n",
    "\n",
    "    for i in tqdm(range(T), desc='Computing Intervals'):\n",
    "        seq_x, seq_y, _, _ = test_dataset[i]\n",
    "        y_true = torch.tensor(seq_y).float().to(device)  # [pred_len, num_features]\n",
    "\n",
    "        # 1. 모델 타입에 따라 입력 데이터 형태를 다르게 설정\n",
    "        if model_type == 'MLP':\n",
    "            x_input = torch.tensor(seq_x).float().flatten().unsqueeze(0).to(device)  # [1, D]\n",
    "        elif model_type == 'DLinear':\n",
    "            x_input = torch.tensor(seq_x).float().unsqueeze(0).to(device)  # [1, seq_len, num_features]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 모델 타입입니다: {model_type}\")\n",
    "\n",
    "        # 모든 부트스트랩 모델을 사용해 예측 (앙상블)\n",
    "        preds = []\n",
    "        for model in bootstrap_models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(x_input).squeeze(0)\n",
    "                # 2. MLP 모델의 출력이 1D일 경우, y_true와 같은 2D 형태로 변환\n",
    "                if model_type == 'MLP' and len(y_pred.shape) == 1:\n",
    "                    y_pred = y_pred.view_as(y_true)\n",
    "                preds.append(y_pred)\n",
    "                \n",
    "        pred_mean = torch.stack(preds).mean(dim=0)  # [pred_len, num_features]\n",
    "\n",
    "\n",
    "        y_true_np = y_true.cpu().numpy()\n",
    "        pred_np = pred_mean.cpu().numpy()\n",
    "        y_true_inv = torch.tensor(train_dataset.inverse_transform(y_true_np)).to(device)\n",
    "        pred_inv = torch.tensor(train_dataset.inverse_transform(pred_np)).to(device)\n",
    "\n",
    "        w_t = torch.quantile(train_residuals, q=(1 - alpha), dim=0)\n",
    "        \n",
    "        upper_interval_t = pred_inv + w_t\n",
    "        lower_interval_t = pred_inv - w_t\n",
    "        \n",
    "\n",
    "        current_test_residual = torch.abs(y_true_inv - pred_inv)\n",
    "\n",
    "\n",
    "        train_residuals = torch.roll(train_residuals, shifts=-1, dims=0)\n",
    "        train_residuals[-1] = current_test_residual\n",
    "\n",
    "        lower_intervals.append(lower_interval_t)\n",
    "        upper_intervals.append(upper_interval_t)\n",
    "        test_preds.append(pred_inv)\n",
    "        test_trues.append(y_true_inv)\n",
    "\n",
    "    return (\n",
    "        torch.stack(upper_intervals),\n",
    "        torch.stack(lower_intervals),\n",
    "        torch.stack(test_preds),\n",
    "        torch.stack(test_trues)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2a81b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Intervals: 100%|██████████| 1756/1756 [00:17<00:00, 99.11it/s] \n",
      "Computing Intervals: 100%|██████████| 1756/1756 [00:26<00:00, 66.86it/s]\n"
     ]
    }
   ],
   "source": [
    "mlp_upper_intervals, mlp_lower_intervals, mlp_test_preds, mlp_test_true = compute_interval(\n",
    "    model_type='MLP',\n",
    "    test_dataset=test_set,\n",
    "    train_dataset=train_set,\n",
    "    train_residuals=var_train_residuals, \n",
    "    bootstrap_models=models,              \n",
    "    alpha=0.05,                          \n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# DLinear 모델에 대한 예측 구간 계산\n",
    "dlinear_upper_intervals, dlinear_lower_intervals, dlinear_test_preds, dlinear_test_true = compute_interval(\n",
    "    model_type='DLinear',\n",
    "    test_dataset=test_set,\n",
    "    train_dataset=train_set,\n",
    "    train_residuals=var_train_residuals,  \n",
    "    bootstrap_models=dlinear_models,      \n",
    "    alpha=0.05,                           \n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75773877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0003, device='cuda:0')\n",
      "tensor(0.0003, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mlp_test_mse = mse(mlp_test_preds, mlp_test_true)\n",
    "dlinear_test_mse = mse(dlinear_test_preds, dlinear_test_true)\n",
    "\n",
    "print(mlp_test_mse)\n",
    "print(dlinear_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da988bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Coverage: 0.9438\n",
      "DLinear Coverage: 0.9438\n"
     ]
    }
   ],
   "source": [
    "def compute_coverage_widths(lower_interval, upper_interval, trues):\n",
    "    \n",
    "    widths = torch.abs(upper_interval.mean(dim=1) - lower_interval.mean(dim=1))\n",
    "    \n",
    "    covered_lower = lower_interval <= trues\n",
    "    covered_upper = trues <= upper_interval\n",
    "    is_covered = covered_lower & covered_upper\n",
    "    num_covered = is_covered.sum().item()\n",
    "    total_samples = trues.numel()\n",
    "    coverage = num_covered / total_samples\n",
    "    \n",
    "    return coverage, widths\n",
    "\n",
    "mlp_coverage, mlp_widths = compute_coverage_widths(mlp_lower_intervals, mlp_upper_intervals, mlp_test_true)\n",
    "dlinear_coverage, dlinear_widths = compute_coverage_widths(dlinear_lower_intervals, dlinear_upper_intervals, dlinear_test_true)\n",
    "\n",
    "\n",
    "print(f'MLP Coverage: {mlp_coverage:.4f}')\n",
    "print(f'DLinear Coverage: {dlinear_coverage:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ae9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_files(assets, model_type, seq_len, pred_len, preds, trues, widths, timestamp, save_paths = './result_enbpi'):\n",
    "#     file_name = f\"{assets}_{model_type}_{seq_len}_{pred_len}.pt\"\n",
    "#     save_path = os.path.join(save_paths, file_name)\n",
    "    \n",
    "#     torch.save({\n",
    "#     'preds': preds,\n",
    "#     'trues': trues,\n",
    "#     'widths': widths,\n",
    "#     'Date':timestamp\n",
    "# }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da29630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def make_files(assets, model_type, seq_len, pred_len, preds, trues, widths, upper, lower, timestamp, save_paths='./result_enbpi'):\n",
    "    file_name = f\"{assets}_{model_type}_{seq_len}_{pred_len}.pkl\"\n",
    "    save_path = os.path.join(save_paths, file_name)\n",
    "\n",
    "    os.makedirs(save_paths, exist_ok=True)  # 경로 없으면 생성\n",
    "\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'preds': preds,       # torch.Tensor\n",
    "            'trues': trues,       # torch.Tensor\n",
    "            'widths': widths,     # torch.Tensor\n",
    "            'Date': timestamp,\n",
    "            'Upper': upper,\n",
    "            'Lower':lower\n",
    "        }, f)\n",
    "\n",
    "    print(f\"Saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ecc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./result_enbpi\\SNP_MLP_60_5.pkl\n"
     ]
    }
   ],
   "source": [
    "make_files(assets='SNP', model_type='MLP', seq_len=train_set[0][0].shape[0], \n",
    "           pred_len=train_set[0][1].shape[0], preds=mlp_test_preds, trues=mlp_test_true, widths=mlp_widths, \n",
    "           upper=mlp_upper_intervals,lower=mlp_lower_intervals, timestamp=timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef479b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./result_enbpi\\SNP_DLinear_60_5.pkl\n"
     ]
    }
   ],
   "source": [
    "make_files(assets='SNP', model_type='DLinear', seq_len=train_set[0][0].shape[0], pred_len=train_set[0][1].shape[0], preds=dlinear_test_preds, \n",
    "           trues=dlinear_test_true, widths=dlinear_widths,  upper=dlinear_upper_intervals,lower=dlinear_lower_intervals,\n",
    "           timestamp=timestamps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conf_preds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
